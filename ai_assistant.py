"""
è„šæœ¬åç§°: AI åŠ©æ‰‹ (ai_assistant.py)

åŠŸèƒ½æè¿°:
    è¿™æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„ AI èŠå¤©æœºå™¨äººï¼Œæ”¯æŒå‘½ä»¤è¡Œç•Œé¢ (CLI) å’Œå›¾å½¢ç”¨æˆ·ç•Œé¢ (Web UI)ã€‚
    å®ƒé€šè¿‡é˜¿é‡Œäº‘çš„é€šä¹‰åƒé—®æ¨¡å‹æä¾›æ™ºèƒ½å¯¹è¯æœåŠ¡ï¼Œå¹¶æ”¯æŒæµå¼å“åº”ä»¥å®ç°å®æ—¶äº¤äº’ã€‚
    CLI æ¨¡å¼ä¸‹å…·æœ‰å¯¹è¯å†å²æŒä¹…åŒ–åŠŸèƒ½ï¼ŒWeb UI æ¨¡å¼åˆ™æä¾›æ›´å‹å¥½çš„å¯è§†åŒ–äº¤äº’ã€‚
    æ­¤å¤–ï¼ŒCLI æ¨¡å¼è¿˜æ”¯æŒå°†å¤–éƒ¨æ–‡ä»¶å†…å®¹æ³¨å…¥åˆ°å¯¹è¯ä¸Šä¸‹æ–‡ä¸­ï¼Œä»¥ä¾¿ AI è¿›è¡Œæ›´æ·±å…¥çš„åˆ†æå’Œè®¨è®ºã€‚

ä½¿ç”¨æ–¹æ³•:
    1.  **CLI æ¨¡å¼ (é»˜è®¤)**:
        åœ¨ç»ˆç«¯ä¸­è¿è¡Œ: `python ai_assistant.py`
        -   è¾“å…¥é—®é¢˜ä¸ AI å¯¹è¯ã€‚
        -   è¾“å…¥ "quit", "exit", "bye", "goodbye" ä¹‹ä¸€å¯ä¿å­˜å¯¹è¯å†å²å¹¶é€€å‡ºã€‚

    2.  **CLI æ¨¡å¼ (å¸¦æ–‡ä»¶æ³¨å…¥)**:
        åœ¨ç»ˆç«¯ä¸­è¿è¡Œ: `python ai_assistant.py <æ–‡ä»¶è·¯å¾„>`
        -   ä¾‹å¦‚: `python ai_assistant.py d:/Documents/Assistant/my_document.txt`
        -   AI ä¼šå…ˆé˜…è¯»æ–‡ä»¶å†…å®¹ï¼Œç„¶åç­‰å¾…ä½ çš„æé—®ã€‚

    3.  **Web UI æ¨¡å¼**:
        åœ¨ç»ˆç«¯ä¸­è¿è¡Œ: `python ai_assistant.py --gui`
        -   è¿™ä¼šå¯åŠ¨ä¸€ä¸ªåŸºäº Gradio çš„ Web ç•Œé¢ï¼Œä½ å¯ä»¥åœ¨æµè§ˆå™¨ä¸­ä¸ AI äº¤äº’ã€‚

é…ç½®:
    -   **API å¯†é’¥**: å¿…é¡»åœ¨é¡¹ç›®æ ¹ç›®å½•çš„ `.env` æ–‡ä»¶ä¸­è®¾ç½® `ALIYUN_API_KEY` ç¯å¢ƒå˜é‡ã€‚
        ä¾‹å¦‚: `ALIYUN_API_KEY="your_api_key_here"`
    -   **ä»£ç†**: å¦‚æœéœ€è¦ï¼Œå¯ä»¥åœ¨ `PROXY_URL` å˜é‡ä¸­é…ç½®ä»£ç†æœåŠ¡å™¨åœ°å€ã€‚
    -   **å†å²è®°å½•**: CLI æ¨¡å¼ä¸‹çš„å¯¹è¯å†å²ä¿å­˜åœ¨ `data/chat_log.json` æ–‡ä»¶ä¸­ã€‚

ä¾èµ–:
    -   `requests`
    -   `gradio`
    -   `python-dotenv`
    -   `json`
    -   `os`
    -   `sys`

æ³¨æ„äº‹é¡¹:
    -   Web UI æ¨¡å¼ä¸‹çš„å¯¹è¯å†å²ä¸ä¼šè¢«æŒä¹…åŒ–ä¿å­˜ã€‚
    -   ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–åº“ (`pip install -r requirements.txt`)ã€‚
"""
import os
import requests
import json
import sys
import gradio as gr
import re # å¯¼å…¥ re æ¨¡å—ç”¨äºæ­£åˆ™è¡¨è¾¾å¼æ“ä½œ
import argparse
from dotenv import load_dotenv

load_dotenv() # åœ¨æ‰€æœ‰ä»£ç ä¹‹å‰ï¼Œè¿è¡Œè¿™ä¸ªå‡½æ•°ï¼Œå®ƒä¼šè‡ªåŠ¨åŠ è½½.envæ–‡ä»¶

# --- 1. é…ç½®ç¨‹åºæ‰€éœ€çš„å˜é‡ ---

# æç¤ºï¼šä¸ºäº†å®‰å…¨ï¼Œæœ€å¥½å°†APIå¯†é’¥å­˜å‚¨åœ¨ç¯å¢ƒå˜é‡ä¸­.å¦‚æœç¯å¢ƒå˜é‡ä¸å­˜åœ¨ï¼Œæ‰“å°é”™è¯¯ä¿¡æ¯å¹¶é€€å‡ºã€‚
API_KEY = os.getenv("ALIYUN_API_KEY") 
if not API_KEY: 
    print("é”™è¯¯ï¼šæœªæ‰¾åˆ°ALIYUN_API_KEYç¯å¢ƒå˜é‡ï¼") 
    print("è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„APIå¯†é’¥") 
    exit(1) 

# å¦‚æœæ— æ³•ç›´æ¥è®¿é—®APIï¼Œå¯ä»¥åœ¨è¿™é‡Œè®¾ç½®ä»£ç†æœåŠ¡å™¨åœ°å€
PROXY_URL = None 
MODEL_NAME = "qwen-flash"
API_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions" 
HISTORY_FILE = "data/chat_log.json" # å†å²è®°å½•æ–‡ä»¶è·¯å¾„
TEMPERARURE = 0.5

# å®šä¹‰æ€»ç»“æç‚¼çš„æ ‡è®°ï¼Œä¸ html_to_md.py ä¸­çš„ SUMMARY_TEMPLATE ä¿æŒä¸€è‡´
SUMMARY_HEADING_MARKER = "# æ€»ç»“æç‚¼\n\n" # æ ‡é¢˜æ ‡è®°ï¼Œåé¢è·Ÿä¸¤ä¸ªæ¢è¡Œç¬¦
SUMMARY_SEPARATOR_MARKER = "\n---" # åˆ†éš”ç¬¦æ ‡è®°ï¼Œå‰é¢è·Ÿä¸€ä¸ªæ¢è¡Œç¬¦
# ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨äºæŸ¥æ‰¾ SUMMARY_HEADING_MARKER ä½œä¸ºæ’å…¥ç‚¹ã€‚re.DOTALL å…è®¸ '.' åŒ¹é…æ¢è¡Œç¬¦ã€‚
SUMMARY_PATTERN = re.compile(rf"({re.escape(SUMMARY_HEADING_MARKER)})", re.DOTALL)
data_folder = os.path.dirname(HISTORY_FILE) # å¦‚æœ data æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œå°±è‡ªåŠ¨åˆ›å»ºå®ƒ
if data_folder and not os.path.exists(data_folder):
    os.makedirs(data_folder)

# --- 2. AI æœåŠ¡ç±» ---
class AIAssistantService:
    """
    å°è£…ä¸ AI æ¨¡å‹äº¤äº’çš„æ‰€æœ‰é€»è¾‘ï¼ŒåŒ…æ‹¬ API è¯·æ±‚ã€æµå¼å“åº”å¤„ç†å’Œé”™è¯¯ç®¡ç†ã€‚
    """
    # --- 2.1. åˆå§‹åŒ–æœåŠ¡ ---
    def __init__(self, api_key, model_name, api_url, temperature, proxy_url=None):
        """
        åˆå§‹åŒ– AI æœåŠ¡å®ä¾‹ã€‚

        :param api_key: ç”¨äº API è®¤è¯çš„å¯†é’¥ã€‚
        :param model_name: è¦ä½¿ç”¨çš„ AI æ¨¡å‹åç§°ã€‚
        :param api_url: API çš„ç»ˆç«¯èŠ‚ç‚¹ URLã€‚
        :param temperature: æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚
        :param proxy_url: (å¯é€‰) ç”¨äºç½‘ç»œè¯·æ±‚çš„ä»£ç†æœåŠ¡å™¨åœ°å€ã€‚
        """
        self.api_key = api_key
        self.model_name = model_name
        self.api_url = api_url
        self.temperature = temperature
        self.proxies = {"http": proxy_url, "https": proxy_url} if proxy_url else None
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }

    # --- 2.2. è·å–æµå¼å›å¤ ---
    def stream_chat_completion(self, history):
        """
        æ¥æ”¶ä¸€ä¸ªå®Œæ•´çš„å¯¹è¯å†å²åˆ—è¡¨ï¼Œä»¥ç”Ÿæˆå™¨çš„æ–¹å¼æµå¼è¿”å› AI çš„å›ç­”ã€‚

        :param history: ä¸€ä¸ªåŒ…å«å¯¹è¯æ¶ˆæ¯çš„åˆ—è¡¨ã€‚
        :return: ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œé€å—(chunk)äº§ç”Ÿ AI çš„å›å¤å†…å®¹ã€‚
        """
        data = {
            "model": self.model_name,
            "messages": history,
            "temperature": self.temperature,
            "stream": True,
        }

        try:
            response = requests.post(self.api_url, headers=self.headers, json=data, proxies=self.proxies, stream=True)
            response.raise_for_status()

            for line in response.iter_lines():
                if line:
                    decoded_line = line.decode('utf-8')
                    if decoded_line.startswith("data:"):
                        json_str = decoded_line[len("data: "):]
                        if json_str.strip() == "[DONE]":
                            break
                        response_json = json.loads(json_str)
                        content = response_json["choices"][0]["delta"].get("content", "")
                        yield content
        except requests.exceptions.RequestException as e:
            yield f"\nå“å‘€ï¼Œç½‘ç»œé”™è¯¯ï¼æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ã€‚é”™è¯¯è¯¦æƒ…ï¼š{e}"
        except Exception as e:
            error_details = response.text if 'response' in locals() else "æ— å“åº”å†…å®¹"
            yield f"å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{e}\næœåŠ¡å™¨å“åº”ï¼š{error_details}"

# --- 3. æ ¸å¿ƒåŠŸèƒ½å°è£… ---

def load_history(file_path):
    """
    ä»æŒ‡å®šè·¯å¾„åŠ è½½å¯¹è¯å†å²ã€‚
    :param file_path: å†å²è®°å½•æ–‡ä»¶çš„è·¯å¾„ã€‚
    :return: ä¸€ä¸ªåŒ…å«å¯¹è¯å†å²çš„åˆ—è¡¨ã€‚å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™è¿”å›ç©ºåˆ—è¡¨ã€‚
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            history = json.load(f)
        print("AIå°åŠ©æ‰‹ï¼šå·²æˆåŠŸåŠ è½½è¿‡å¾€è®°å¿†ã€‚")
        return history
    except FileNotFoundError:
        print("AIå°åŠ©æ‰‹ï¼šä½ å¥½ï¼ä¸€ä¸ªæ–°çš„æ—…ç¨‹å¼€å§‹äº†ã€‚")
        return []

def save_history(history, file_path):
    """
    å°†å¯¹è¯å†å²ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„ã€‚
    :param history: è¦ä¿å­˜çš„å¯¹è¯å†å²åˆ—è¡¨ã€‚
    :param file_path: å†å²è®°å½•æ–‡ä»¶çš„è·¯å¾„ã€‚
    """
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        print("AIå°åŠ©æ‰‹ï¼šè®°å¿†å·²ä¿å­˜ï¼ŒæœŸå¾…ä¸‹æ¬¡ä¸ä½ ç›¸è§ï¼")
    except Exception as e:
        print(f"AIå°åŠ©æ‰‹ï¼šå“å‘€ï¼Œä¿å­˜è®°å¿†æ—¶å‡ºé”™äº†ï¼š{e}")

# --- æ‰¹å¤„ç†æ–‡ä»¶å¤¹æ€»ç»“åŠŸèƒ½ ---
def process_folder_for_summaries(folder_path, ai_service, prompt_template):
    """
    éå†æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰Markdown/æ–‡æœ¬æ–‡ä»¶ï¼ŒæŸ¥æ‰¾æ€»ç»“æç‚¼åŒºåŸŸï¼Œ
    å¦‚æœè¯¥åŒºåŸŸä¸ºç©ºï¼Œåˆ™è°ƒç”¨AIè¿›è¡Œæ€»ç»“å¹¶å†™å…¥æ–‡ä»¶ã€‚
    å¦‚æœè¯¥åŒºåŸŸå·²æœ‰å†…å®¹ï¼Œåˆ™å°†æ–°æ€»ç»“å†™å…¥ä¸€ä¸ªæ–°æ–‡ä»¶ã€‚
    :param folder_path: è¦å¤„ç†çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚
    :param ai_service: AI æœåŠ¡å®ä¾‹ã€‚
    :param prompt_template: ç”¨äºç”ŸæˆAIè¯·æ±‚çš„æç¤ºè¯æ¨¡æ¿ã€‚
    """
    print(f"ğŸ“ æ£€æµ‹åˆ°æ–‡ä»¶å¤¹è¾“å…¥ï¼š'{folder_path}'ã€‚å°†è¿›å…¥æ‰¹å¤„ç†æ¨¡å¼ï¼Œè‡ªåŠ¨æ€»ç»“æ–‡ä»¶ã€‚")
    processed_count = 0 # ç»Ÿè®¡æˆåŠŸå¤„ç†ï¼ˆåŸåœ°æ›´æ–°ï¼‰çš„æ–‡ä»¶æ•°é‡
    skipped_count = 0
    error_count = 0

    if "{content}" not in prompt_template:
        print("âš ï¸ è­¦å‘Šï¼šæä¾›çš„æç¤ºè¯æ¨¡æ¿ä¸­æœªæ‰¾åˆ° '{content}' å ä½ç¬¦ã€‚AIå¯èƒ½æ— æ³•è·å–æ–‡ä»¶å†…å®¹ã€‚")

    for root, _, files in os.walk(folder_path):
        for file_name in files:
            # åªå¤„ç† Markdown å’Œæ–‡æœ¬æ–‡ä»¶
            if not file_name.lower().endswith(('.md')):
                continue

            file_path = os.path.join(root, file_name)
            print(f"\n--- æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_name} ---")

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ€»ç»“æç‚¼åŒºåŸŸ
                match = SUMMARY_PATTERN.search(content)

                if not match: # å¦‚æœè¿æ ‡é¢˜æ ‡è®°éƒ½æ‰¾ä¸åˆ°ï¼Œå°±è·³è¿‡
                    print(f"   â­ï¸ è·³è¿‡ '{file_name}'ï¼šæœªæ‰¾åˆ°æ€»ç»“æç‚¼çš„æ ‡é¢˜æ ‡è®° ('{SUMMARY_HEADING_MARKER.strip()}')ã€‚")
                    skipped_count += 1
                    continue

                # å‡†å¤‡å‘é€ç»™ AI çš„æç¤ºè¯
                summary_prompt = prompt_template.format(content=content) # å°†æ–‡ä»¶å†…å®¹å¡«å……åˆ°æç¤ºè¯æ¨¡æ¿ä¸­
                # ä¸º AI è°ƒç”¨åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„å¯¹è¯å†å²ï¼Œå› ä¸ºæ€»ç»“ä¸éœ€è¦ä¹‹å‰çš„ä¸Šä¸‹æ–‡
                temp_history = [{"role": "user", "content": summary_prompt}]
                
                print("   ğŸ¤– æ­£åœ¨è¯·æ±‚ AI ç”Ÿæˆå†…å®¹...")
                ai_summary = ""
                for chunk in ai_service.stream_chat_completion(temp_history):
                    ai_summary += chunk
                    # å¯ä»¥åœ¨è¿™é‡Œæ‰“å° chunk ä»¥æä¾›å®æ—¶åé¦ˆï¼Œä½†æ‰¹å¤„ç†æ¨¡å¼ä¸‹é€šå¸¸ä¸éœ€è¦
                    # print(chunk, end="", flush=True)

                if not ai_summary.strip():
                    print(f"   â­ï¸ AI æœªè¿”å›æœ‰æ•ˆå†…å®¹ï¼Œè·³è¿‡ '{file_name}'ã€‚è¯·æ£€æŸ¥æç¤ºè¯æˆ–æ–‡ä»¶å†…å®¹ã€‚")
                    skipped_count += 1
                    continue

                # ç®€åŒ–é€»è¾‘ï¼šç›´æ¥åœ¨ SUMMARY_HEADING_MARKER ä¹‹åæ’å…¥ AI æ€»ç»“
                # r"\1" æ˜¯æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…åˆ°çš„ SUMMARY_HEADING_MARKER æœ¬èº«
                # ai_summary.strip() æ˜¯ AI ç”Ÿæˆçš„æ€»ç»“å†…å®¹
                # "\n\n" æ˜¯ä¸ºäº†åœ¨æ’å…¥çš„æ€»ç»“å’ŒåŸæœ‰å†…å®¹ä¹‹é—´ä¿æŒä¸¤è¡Œç©ºè¡Œï¼Œä»¥ç¬¦åˆMarkdownæ ¼å¼å’Œå¯è¯»æ€§
                new_content = SUMMARY_PATTERN.sub(r"\1" + ai_summary.strip() + "\n\n", content, 1)

                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                print(f"   âœ… æˆåŠŸæ’å…¥å†…å®¹å¹¶æ›´æ–°æ–‡ä»¶: '{file_name}'")
                processed_count += 1
                    
            except Exception as e:
                print(f"   âŒ å¤„ç†æ–‡ä»¶ '{file_name}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                error_count += 1
                continue
    
    print("\n--- æ‰¹å¤„ç†å®Œæˆ ---")
    total_files = processed_count + skipped_count + error_count
    print(f"æ€»è®¡æ‰«ææ–‡ä»¶: {total_files}")
    print(f"æˆåŠŸæ’å…¥å†…å®¹å¹¶æ›´æ–°: {processed_count}")
    print(f"è·³è¿‡ (æ— æ ‡è®°åŒºåŸŸ): {skipped_count}")
    print(f"å¤„ç†å¤±è´¥: {error_count}")
    print("------------------")
    
# --- 4. å‘½ä»¤è¡Œç•Œé¢ (CLI) å¯åŠ¨é€»è¾‘ ---
def start_cli():
    """å¯åŠ¨å‘½ä»¤è¡Œç‰ˆæœ¬çš„ AI åŠ©æ‰‹ã€‚"""
    # --- 1. ä½¿ç”¨ argparse è§£æå‘½ä»¤è¡Œå‚æ•° ---
    parser = argparse.ArgumentParser(
        description="ä¸€ä¸ªæ”¯æŒå¤šç§è®°å¿†æ¨¡å¼å’Œæ–‡ä»¶æ³¨å…¥çš„å‘½ä»¤è¡Œ AI åŠ©æ‰‹ã€‚",
        # formatter_class å¯ä»¥è®©å¸®åŠ©ä¿¡æ¯æ›´å¥½åœ°æ˜¾ç¤ºé»˜è®¤å€¼
        formatter_class=argparse.ArgumentDefaultsHelpFormatter 
    )
    parser.add_argument(
        'memory_mode',
        nargs='?', # '?' è¡¨ç¤º 0 æˆ– 1 ä¸ªå‚æ•°ï¼Œä½¿å…¶æˆä¸ºå¯é€‰çš„ä½ç½®å‚æ•°
        choices=['no', 'short', 'long'],
        default='short',
        help="è®¾ç½®è®°å¿†æ¨¡å¼: 'no' (æ— è®°å¿†), 'short' (çŸ­æœŸä¼šè¯è®°å¿†), 'long' (é•¿æœŸæŒä¹…åŒ–è®°å¿†)ã€‚"
    )
    parser.add_argument(
        '-f', '--file',
        dest='file_path', # è§£æåçš„å‚æ•°å
        default=None,
        help="æŒ‡å®šè¦åŠ è½½åˆ°ä¸Šä¸‹æ–‡ä¸­çš„æ–‡ä»¶è·¯å¾„ã€‚"
    )
    # åˆ›å»ºä¸€ä¸ªäº’æ–¥ç»„ï¼Œç¡®ä¿ --prompt å’Œ --prompt-file ä¸ä¼šåŒæ—¶ä½¿ç”¨
    prompt_group = parser.add_mutually_exclusive_group()
    prompt_group.add_argument(
        '--prompt',
        dest='prompt_string',
        help="ç›´æ¥åœ¨å‘½ä»¤è¡Œä¸­æä¾›æç¤ºè¯æ¨¡æ¿ã€‚è¯·ä½¿ç”¨ '{content}' ä½œä¸ºæ–‡ä»¶å†…å®¹çš„å ä½ç¬¦ã€‚"
    )
    prompt_group.add_argument(
        '--prompt-file',
        dest='prompt_file_path',
        help="ä»æŒ‡å®šæ–‡ä»¶ä¸­åŠ è½½æç¤ºè¯æ¨¡æ¿ã€‚æ–‡ä»¶ä¸­åº”åŒ…å« '{content}' å ä½ç¬¦ã€‚"
    )
    # ä» sys.argv ä¸­è¿‡æ»¤æ‰è„šæœ¬åå’Œ '--gui' æ ‡å¿—ï¼Œåªè§£æä¸CLIç›¸å…³çš„å‚æ•°
    cli_args = [arg for arg in sys.argv[1:] if arg != '--gui']
    args = parser.parse_args(cli_args)

    print("ğŸš€ æ­£åœ¨å¯åŠ¨å‘½ä»¤è¡Œ AI åŠ©æ‰‹...")
    print(f"ğŸ§  è®°å¿†æ¨¡å¼: {args.memory_mode}")

    # --- 2. åˆå§‹åŒ–æœåŠ¡å’Œä¼šè¯çŠ¶æ€ ---
    ai_service = AIAssistantService(
        api_key=API_KEY,
        model_name=MODEL_NAME,
        api_url=API_URL,
        temperature=TEMPERARURE,
        proxy_url=PROXY_URL
    )

    # æ ¹æ®è®°å¿†æ¨¡å¼åˆå§‹åŒ–å¯¹è¯å†å²
    if args.memory_mode == 'long':
        conversation_history = load_history(HISTORY_FILE)
    else:
        conversation_history = []
        print("AIå°åŠ©æ‰‹ï¼šä½ å¥½ï¼ä¸€ä¸ªæ–°çš„æ—…ç¨‹å¼€å§‹äº†ã€‚")

    file_context = None
    if args.file_path:
        if os.path.isdir(args.file_path):
            # --- æ‰¹å¤„ç†æ¨¡å¼çš„æç¤ºè¯å¤„ç† ---
            prompt_template = ""
            if args.prompt_string:
                prompt_template = args.prompt_string
            elif args.prompt_file_path:
                try:
                    with open(args.prompt_file_path, 'r', encoding='utf-8') as f:
                        prompt_template = f.read()
                except Exception as e:
                    print(f"âŒ è¯»å–æç¤ºè¯æ–‡ä»¶ '{args.prompt_file_path}' æ—¶å‡ºé”™: {e}")
                    sys.exit(1)
            else:
                # å¦‚æœç”¨æˆ·æœªæä¾›æç¤ºè¯ï¼Œåˆ™è¿›å…¥äº¤äº’å¼è¾“å…¥
                print("\næ‰¹å¤„ç†æ¨¡å¼éœ€è¦ä¸€ä¸ªæç¤ºè¯æ¨¡æ¿ã€‚")
                print("æ¨¡æ¿ä¸­å¿…é¡»åŒ…å« '{content}' å ä½ç¬¦ï¼Œå®ƒå°†è¢«æ›¿æ¢ä¸ºæ¯ä¸ªæ–‡ä»¶çš„å®é™…å†…å®¹ã€‚")
                default_prompt = "è¯·ä½ ä»”ç»†é˜…è¯»ä»¥ä¸‹æ–‡æœ¬ï¼Œå¹¶æç‚¼å‡ºä¸»è¦å†…å®¹å’Œå…³é”®ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½ç®€æ´çš„æ€»ç»“ã€‚è¯·ç›´æ¥è¾“å‡ºæ€»ç»“å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„å‰ç¼€æˆ–åç¼€ã€‚\n\næ–‡æœ¬å†…å®¹:\n```\n{content}\n```"
                print(f"\nç¤ºä¾‹ (é»˜è®¤æ¨¡æ¿):\n---\n{default_prompt}\n---")
                
                user_prompt = input("\nè¯·è¾“å…¥ä½ çš„æç¤ºè¯æ¨¡æ¿ (ç›´æ¥æŒ‰ Enter ä½¿ç”¨é»˜è®¤æ¨¡æ¿): \n")
                if user_prompt.strip():
                    prompt_template = user_prompt
                else:
                    prompt_template = default_prompt
            
            process_folder_for_summaries(args.file_path, ai_service, prompt_template)
            sys.exit(0) # æ‰¹å¤„ç†å®Œæˆåé€€å‡ºç¨‹åº
        elif os.path.isfile(args.file_path):
            # å¦‚æœæ˜¯æ–‡ä»¶ï¼ŒåŠ è½½æ–‡ä»¶å†…å®¹ä½œä¸ºå¯¹è¯ä¸Šä¸‹æ–‡
            try:
                with open(args.file_path, 'r', encoding='utf-8') as f:
                    file_context = f.read()
                print(f"ğŸ“ å·²åŠ è½½æ–‡ä»¶ '{os.path.basename(args.file_path)}'ã€‚ç°åœ¨æ‚¨å¯ä»¥åŸºäºè¯¥æ–‡ä»¶æé—®äº†ã€‚")
            except FileNotFoundError:
                print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {args.file_path}ã€‚è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
                sys.exit(1)
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
                sys.exit(1)
        else:
            print(f"âŒ é”™è¯¯ï¼š'{args.file_path}' æ—¢ä¸æ˜¯æ–‡ä»¶ä¹Ÿä¸æ˜¯æ–‡ä»¶å¤¹ã€‚è¯·æä¾›æœ‰æ•ˆè·¯å¾„ã€‚")
            sys.exit(1)

    # ä½¿ç”¨ while True åˆ›å»ºä¸€ä¸ªæ— é™å¾ªç¯ï¼ŒæŒç»­æ¥æ”¶ç”¨æˆ·è¾“å…¥
    while True:
        # ä½¿ç”¨ input() æ¥è·å–ä½ åœ¨ç»ˆç«¯è¾“å…¥çš„é—®é¢˜
        user_input = input("ä½ ï¼š")

        # è®¾ç½®é€€å‡ºæ¡ä»¶ï¼šå½“ç”¨æˆ·è¾“å…¥ç‰¹å®šè¯æ±‡æ—¶ï¼Œä¿å­˜å†å²å¹¶é€€å‡ºå¾ªç¯
        # .lower() å°†è¾“å…¥è½¬ä¸ºå°å†™ï¼Œä½¿å¾—åˆ¤æ–­ä¸åŒºåˆ†å¤§å°å†™
        if user_input.lower() in ["quit", "exit","bye","goodbye","q","e"]:
            # ä»…åœ¨é•¿æœŸè®°å¿†æ¨¡å¼ä¸‹ä¿å­˜å†å²
            if args.memory_mode == 'long':
                save_history(conversation_history, HISTORY_FILE)
            print("AIå°åŠ©æ‰‹ï¼šæœŸå¾…ä¸‹æ¬¡ä¸ä½ ç›¸è§ï¼")
            break

        # --- æ ¸å¿ƒä¿®æ”¹ï¼šåŠ¨æ€æ„å»ºç”¨æˆ·è¾“å…¥ ---
        # å¦‚æœå­˜åœ¨æ–‡ä»¶ä¸Šä¸‹æ–‡ï¼Œåˆ™å°†å…¶ä¸ç”¨æˆ·å½“å‰é—®é¢˜ç»„åˆ
        if file_context:
            # æ„å»ºä¸€ä¸ªåŒ…å«æ–‡ä»¶ä¸Šä¸‹æ–‡å’Œç”¨æˆ·é—®é¢˜çš„å¤åˆæç¤º
            final_input = f"""è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹æ¥å›ç­”æˆ‘çš„é—®é¢˜ã€‚
---
æ–‡æ¡£å†…å®¹:
{file_context}
---
æˆ‘çš„é—®é¢˜æ˜¯ï¼š{user_input}
"""
        else:
            # å¦‚æœæ²¡æœ‰æ–‡ä»¶ä¸Šä¸‹æ–‡ï¼Œåˆ™ç›´æ¥ä½¿ç”¨ç”¨æˆ·è¾“å…¥
            final_input = user_input

        # æ— è®ºä½•ç§æ¨¡å¼ï¼Œéƒ½å°†ç”¨æˆ·çš„è¾“å…¥å­˜å…¥å®Œæ•´çš„å†å²è®°å½•ï¼Œä»¥å¤‡å°†æ¥ä¿å­˜
        conversation_history.append({"role": "user", "content": final_input})

        # --- 3. æ ¹æ®è®°å¿†æ¨¡å¼å†³å®šå‘é€ç»™ AI çš„å†…å®¹ ---
        if args.memory_mode == 'no':
            # æ— è®°å¿†æ¨¡å¼ï¼šåªå‘é€åŒ…å«å½“å‰è¿™ä¸€æ¬¡è¾“å…¥çš„æ–°åˆ—è¡¨
            history_to_send = [conversation_history[-1]]
        else: # 'short' å’Œ 'long' æ¨¡å¼éƒ½ä½¿ç”¨çŸ­æœŸè®°å¿†
            # çŸ­æœŸ/é•¿æœŸè®°å¿†æ¨¡å¼ï¼šå‘é€åŒ…å«æ‰€æœ‰å†å²è®°å½•çš„å®Œæ•´åˆ—è¡¨
            history_to_send = conversation_history

        # è°ƒç”¨ç”Ÿæˆå™¨å‡½æ•°ï¼Œå¹¶è¿­ä»£æ‰“å°ç»“æœ
        print(f"AIåŠ©æ‰‹ï¼š", end="")
        full_response = ""
        has_error = False
        # è°ƒç”¨ AI æœåŠ¡å®ä¾‹çš„æ–¹æ³•
        for chunk in ai_service.stream_chat_completion(history_to_send):
            # æ£€æŸ¥è¿”å›çš„ç‰‡æ®µä¸­æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
            if "ç½‘ç»œé”™è¯¯" in chunk or "æœªçŸ¥é”™è¯¯" in chunk:
                has_error = True
            full_response += chunk
            # flush=True å¼ºåˆ¶åˆ·æ–°è¾“å‡ºç¼“å†²åŒºï¼Œç¡®ä¿å†…å®¹èƒ½è¢«ç«‹å³æ˜¾ç¤º
            print(chunk, end="", flush=True)
        print() # ç»“æŸæ—¶æ¢è¡Œ

        # æ— è®ºä½•ç§æ¨¡å¼ï¼Œéƒ½å°†AIçš„å›ç­”ä¹Ÿå­˜å…¥å®Œæ•´çš„å†å²è®°å½•
        # (ç¡®ä¿ä¸ä¼šæŠŠé”™è¯¯ä¿¡æ¯ä¹Ÿè®°ä¸‹æ¥), ä»¥å¤‡å°†æ¥ä¿å­˜
        if not has_error:
            conversation_history.append({"role": "assistant", "content": full_response})
        
        print("\n" + "-"*30) #æ‰“å°åˆ†éš”çº¿ï¼Œå¹¶åœ¨å‰é¢åŠ ä¸€ä¸ªæ¢è¡Œä»¥æ”¹å–„é—´è·

# --- 5. å›¾å½¢ç”¨æˆ·ç•Œé¢ (GUI) å¯åŠ¨é€»è¾‘ ---
def start_gui():
    """å¯åŠ¨ Gradio å›¾å½¢ç”¨æˆ·ç•Œé¢ã€‚"""
    print("ğŸš€ æ­£åœ¨å¯åŠ¨ Gradio å›¾å½¢ç•Œé¢...")

    # --- æ–°å¢ï¼šåˆå§‹åŒ– AI æœåŠ¡ ---
    ai_service = AIAssistantService(
        api_key=API_KEY,
        model_name=MODEL_NAME,
        api_url=API_URL,
        temperature=TEMPERARURE,
        proxy_url=PROXY_URL
    )

    # --- ä¸º Gradio UI ç¼–å†™çš„æ¥å£å‡½æ•° ---
    def chat_response(user_input, chatbot_history, conversation_state):
        """å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œå¹¶æµå¼è¿”å›AIå“åº”"""
        # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²çŠ¶æ€
        conversation_state.append({"role": "user", "content": user_input})
        # æ›´æ–°Chatbot UIä»¥ç«‹å³æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
        chatbot_history.append([user_input, ""])
        # yield å…³é”®å­—ä½¿è¿™ä¸ªå‡½æ•°æˆä¸ºä¸€ä¸ªç”Ÿæˆå™¨ï¼Œå¯ä»¥é€æ­¥è¿”å›UIæ›´æ–°
        yield chatbot_history, conversation_state

        # æµå¼è·å–AIå›å¤
        full_response = ""
        has_error = False
        # è°ƒç”¨ AI æœåŠ¡å®ä¾‹çš„æ–¹æ³•
        for chunk in ai_service.stream_chat_completion(conversation_state):
            if "ç½‘ç»œé”™è¯¯" in chunk or "æœªçŸ¥é”™è¯¯" in chunk:
                has_error = True
            full_response += chunk
            chatbot_history[-1][1] = full_response # æ›´æ–°èŠå¤©æœºå™¨äººç•Œé¢ä¸­æœ€åä¸€æ¡æ¶ˆæ¯çš„AIå›å¤éƒ¨åˆ†
            yield chatbot_history, conversation_state

        # å¦‚æœæ²¡æœ‰é”™è¯¯ï¼Œå°†å®Œæ•´çš„AIå›å¤æ·»åŠ åˆ°å¯¹è¯å†å²çŠ¶æ€
        # æ³¨æ„ï¼šGradioç‰ˆæœ¬ä¸­ï¼Œå†å²è®°å½•æ˜¯ä¸´æ—¶çš„ï¼Œåªåœ¨å½“å‰ä¼šè¯ä¸­æœ‰æ•ˆï¼Œå…³é—­å³ä¸¢å¤±
        if not has_error:
            conversation_state.append({"role": "assistant", "content": full_response})

    # --- æ„å»º Gradio ç•Œé¢ ---
    with gr.Blocks(title="AI åŠ©æ‰‹") as app:
        # gr.State ç”¨äºåœ¨åç«¯å­˜å‚¨ä¼šè¯æœŸé—´çš„å®Œæ•´å¯¹è¯å†å²ï¼ˆåŒ…å«system roleç­‰ï¼‰
        # å®ƒåœ¨å‰ç«¯æ˜¯ä¸å¯è§çš„
        conversation_state = gr.State(value=[])

        gr.Markdown("# ğŸ¤– AI åŠ©æ‰‹")
        gr.Markdown("ä¸€ä¸ªç”±é˜¿é‡Œé€šä¹‰åƒé—®é©±åŠ¨çš„æ™ºèƒ½åŠ©æ‰‹ã€‚")

        # ä¸»è¦èŠå¤©ç•Œé¢
        chatbot = gr.Chatbot(label="é€šä¹‰åƒé—®", height=500)
        
        with gr.Row():
            txt_input = gr.Textbox(show_label=False, lines=3, placeholder="è¯¢é—®ä»»ä½•é—®é¢˜", scale=8)
            btn_submit = gr.Button("å‘é€", variant="primary", scale=1)

        # --- ç»‘å®šäº‹ä»¶ ---
        # å°†æäº¤åŠ¨ä½œï¼ˆæŒ‰å›è½¦æˆ–ç‚¹å‡»æŒ‰é’®ï¼‰ç»‘å®šåˆ° chat_response å‡½æ•°
        txt_input.submit(chat_response, [txt_input, chatbot, conversation_state], [chatbot, conversation_state]).then(lambda: "", [], [txt_input])
        btn_submit.click(chat_response, [txt_input, chatbot, conversation_state], [chatbot, conversation_state]).then(lambda: "", [], [txt_input])

    # å¯åŠ¨Gradioåº”ç”¨
    app.launch()

# --- 6. ä¸»ç¨‹åºæ‰§è¡Œå…¥å£ ---
# ä¸‹é¢çš„ä»£ç åªæœ‰åœ¨ç›´æ¥è¿è¡Œ `python ai_assistant.py` æ—¶æ‰ä¼šæ‰§è¡Œ
if __name__ == "__main__":
    # ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦è¦å¯åŠ¨ GUI æ¨¡å¼
    if '--gui' in sys.argv:
        start_gui()
    else:
        # å¦åˆ™ï¼Œå¯åŠ¨é»˜è®¤çš„å‘½ä»¤è¡Œæ¨¡å¼
        # start_cli å‡½æ•°å†…éƒ¨ä¼šä½¿ç”¨ argparse å¤„ç†æ‰€æœ‰ç›¸å…³çš„å‘½ä»¤è¡Œå‚æ•°
        start_cli()
