
# å¯¼å…¥æˆ‘ä»¬éœ€è¦çš„åº“
import asyncio  # å¯¼å…¥ asyncio åº“ï¼Œå› ä¸º Playwright æ˜¯åŸºäºå¼‚æ­¥ I/O çš„ï¼Œéœ€è¦å®ƒæ¥è¿è¡Œ
import argparse # å¯¼å…¥ argparse åº“ä»¥å¤„ç†å‘½ä»¤è¡Œå‚æ•°
from playwright.async_api import async_playwright # ä» playwright åº“ä¸­å¯¼å…¥å¼‚æ­¥ API
from bs4 import BeautifulSoup # å¯¼å…¥ BeautifulSoup ç”¨äºè§£æ HTML
from markdownify import markdownify # å¯¼å…¥ markdownify ç”¨äºå°† HTML è½¬ä¸º Markdown
import os # å¯¼å…¥ os åº“ï¼Œç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„
import re # å¯¼å…¥ re åº“ï¼Œç”¨äºæ­£åˆ™è¡¨è¾¾å¼æ“ä½œï¼Œä»¥å‡€åŒ–æ–‡ä»¶å
from bs4.element import Tag # å¯¼å…¥ Tag ç±»å‹ç”¨äºç±»å‹æç¤º
from datetime import datetime # å¯¼å…¥ datetime ç”¨äºè·å–å½“å‰æ—¶é—´
from readability import Document # å¯¼å…¥ readability åº“ï¼Œç”¨äºæ™ºèƒ½æå–æ–‡ç« æ­£æ–‡
from urllib.parse import urljoin # å¯¼å…¥ urljoin ç”¨äºå¤„ç†ç›¸å¯¹ URL è·¯å¾„

# --- å…¨å±€å¸¸é‡ ---
# å®šä¹‰ä¸€ä¸ªå¸¸é‡å­—ç¬¦ä¸²ï¼Œç”¨äºåœ¨ Front Matter ä¹‹åã€æ­£æ–‡ä¹‹å‰æ’å…¥çš„æ€»ç»“æç‚¼æ¨¡æ¿
SUMMARY_TEMPLATE = "# æ€»ç»“æç‚¼\n\n\n---\n\n\n"


# --- 1. é€šè¿‡playwrightæŠ“å–HTMLå†…å®¹ ---
async def fetch_html_from_url(url: str) -> str | None:
    """
    ä½¿ç”¨ Playwright å¼‚æ­¥æŠ“å–æŒ‡å®š URL çš„ HTML å†…å®¹ã€‚
    :param url: ç›®æ ‡ç½‘é¡µçš„ URLã€‚
    :return: æˆåŠŸæ—¶è¿”å› HTML å­—ç¬¦ä¸²ï¼Œå¤±è´¥æ—¶è¿”å› Noneã€‚
    """
    print("ğŸš€ è„šæœ¬å¯åŠ¨ï¼Œå‡†å¤‡è¿æ¥ Playwright...")
    
    # é€šè¿‡async with è¯­å¥æ¥ç®¡ç† Playwright çš„ç”Ÿå‘½å‘¨æœŸï¼Œç¡®ä¿æµè§ˆå™¨è¢«æ­£ç¡®å…³é—­
    # asyncè¡¨ç¤ºä»£ç å¯ä»¥å¼‚æ­¥å¤„ç†ï¼Œæ‰€è°“å¼‚æ­¥ï¼Œæ˜¯æŒ‡å¯åœ¨æ‰§è¡Œä¸­æš‚åœï¼Œç­‰å¾…èµ„æºåˆ°ä½å†é‡å¯
    # withè¡¨ç¤ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œæ‰€è°“ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œæ˜¯æŒ‡è‡ªåŠ¨å¤„ç†ç›¸å…³åå°èµ„æºçš„å¼€å¯å’Œé‡Šæ”¾ï¼Œå¦‚æ–‡ä»¶ã€ç½‘ç»œè¿æ¥ç­‰
    # async_playwright()æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå¯åŠ¨ç›¸å…³èµ„æºï¼Œè¿”å›ä¸€ä¸ªPlaywright ä¸»æ§åˆ¶å¯¹è±¡ç»™p
    async with async_playwright() as p:
        try:
            # å¼‚æ­¥ï¼ˆå¯awaitï¼‰å¯åŠ¨ä¸€ä¸ª Chromium æµè§ˆå™¨å®ä¾‹ï¼ˆBrowserTypeå¯¹è±¡ï¼‰ã€‚headless=True è¡¨ç¤ºåœ¨åå°è¿è¡Œï¼Œä¸æ˜¾ç¤ºæµè§ˆå™¨çª—å£ã€‚
            # ä½ ä¹Ÿå¯ä»¥æ¢æˆ p.firefox.launch() æˆ– p.webkit.launch()
            # è¯¥æ–¹æ³•è¿”å›ä¸€ä¸ªBrowserå¯¹è±¡ï¼Œèµ‹å€¼ç»™browser
            browser = await p.chromium.launch(headless=True)
            print("âœ… æµè§ˆå™¨å·²å¯åŠ¨")

            # åœ¨æµè§ˆå™¨ä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„é¡µé¢ï¼ˆPageå¯¹è±¡ï¼‰ï¼Œå¹¶è®¾ç½®ä¸€ä¸ªçœŸå®çš„ User-Agent æ¥æ¨¡æ‹Ÿæ™®é€šç”¨æˆ·ï¼Œé˜²æ­¢åŸºç¡€çš„åçˆ¬è™«æ£€æµ‹ã€‚
            # user_agent æ˜¯ browser.new_page æ–¹æ³•çš„ä¸€ä¸ªå…³é”®å­—å‚æ•°ï¼ˆkeyword argumentï¼‰ã€‚
            page = await browser.new_page(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
            )
            print("âœ… é¡µé¢å·²åˆ›å»ºï¼Œå¹¶è®¾ç½®äº†è‡ªå®šä¹‰ User-Agent")
            print(f"ğŸŒ æ­£åœ¨å¯¼èˆªåˆ°: {url}")

            # è®¿é—®æˆ‘ä»¬æƒ³è¦æŠ“å–çš„ URLï¼Œå¹¶ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            # await å…³é”®å­—è¡¨ç¤ºâ€œç­‰å¾…è¿™ä¸ªæ“ä½œå®Œæˆå†ç»§ç»­â€
            # å¢åŠ  timeout å‚æ•°ï¼Œå°†é»˜è®¤çš„30ç§’è¶…æ—¶å»¶é•¿åˆ°60ç§’ (60000æ¯«ç§’)
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)
            print("âœ… é¡µé¢åŠ è½½å®Œæˆ")

            # è·å–å½“å‰é¡µé¢çš„å®Œæ•´ HTML å†…å®¹ï¼ˆä¹Ÿå°±æ˜¯html_contentï¼‰
            html_content = await page.content()
            print("ğŸ“„ å·²è·å–é¡µé¢ HTML å†…å®¹")

            # æ“ä½œå®Œæˆåï¼Œå…³é—­æµè§ˆå™¨
            await browser.close()
            print("âœ… æµè§ˆå™¨å·²å…³é—­")
            return html_content
        except Exception as e:
            print(f"âŒ åœ¨ä½¿ç”¨ Playwright æŠ“å–ç½‘é¡µæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None


# --- 2-1. å…ƒæ•°æ®æå– ---
def _extract_head_metadata(soup: BeautifulSoup) -> dict:
    """ä» HTML çš„ <head> éƒ¨åˆ†æå–é€šç”¨çš„å…ƒæ•°æ®ã€‚"""
    metadata = {}
    # å®šä¹‰å…ƒæ•°æ®æå–è§„åˆ™ï¼š(å…ƒæ•°æ®é”®å, CSSé€‰æ‹©å™¨, å±æ€§å)
    rules = [
        ('description', 'meta[name="description"]', 'content'),
        ('description', 'meta[property="og:description"]', 'content'),
        ('site_name', 'meta[property="og:site_name"]', 'content'),
    ]

    for key, selector, attr in rules:
        # å¦‚æœè¿™ä¸ªå…ƒæ•°æ®è¿˜æ²¡è¢«æ‰¾åˆ°ï¼Œå°±å°è¯•æŸ¥æ‰¾
        if key not in metadata or not metadata[key]:
            element = soup.select_one(selector)
            # å¢åŠ å¥å£®æ€§æ£€æŸ¥ï¼šç¡®ä¿ element å­˜åœ¨ï¼Œå¹¶ä¸”å¯¹åº”çš„å±æ€§ä¹Ÿå­˜åœ¨
            if element:
                value = element.get(attr)
                if value:
                    # ä½¿ç”¨ str() å°†è·å–åˆ°çš„å€¼ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰æ˜¾å¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç„¶åå†è°ƒç”¨ strip()
                    metadata[key] = str(value).strip()
    return metadata

def _process_wechat_html(soup: BeautifulSoup) -> tuple[Tag | None, dict]:
    """ä¸“é—¨å¤„ç†å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çš„HTMLï¼Œæå–å…ƒæ•°æ®å’Œæ­£æ–‡ã€‚"""
    print("ğŸ’¡ æ£€æµ‹åˆ°å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œå¯åŠ¨ä¸“ç”¨å¤„ç†å™¨...")
    # é¦–å…ˆï¼Œå°è¯•ä» head ä¸­æå–é€šç”¨å…ƒæ•°æ®ä½œä¸ºåŸºç¡€
    metadata = _extract_head_metadata(soup)
    
    # ç„¶åï¼Œä½¿ç”¨å¾®ä¿¡ç‰¹å®šçš„é€‰æ‹©å™¨æ¥è¦†ç›–å’Œè¡¥å……å…ƒæ•°æ®ï¼Œå› ä¸ºå®ƒä»¬æ›´å‡†ç¡®
    title_element = soup.select_one('h1#activity-name')
    if title_element:
        metadata["title"] = title_element.get_text(strip=True)
        print(f"   âœ’ï¸ æˆåŠŸæå–å¾®ä¿¡æ ‡é¢˜: {metadata['title']}")

    author_element = soup.select_one('#js_name')
    if author_element:
        metadata["author"] = author_element.get_text(strip=True)
        print(f"   ğŸ‘¤ æˆåŠŸæå–å¾®ä¿¡ä½œè€…: {metadata['author']}")

    published_element = soup.select_one('em#publish_time')
    if published_element:
        metadata["published"] = published_element.get_text(strip=True)
        print(f"   ğŸ“… æˆåŠŸæå–å¾®ä¿¡å‘å¸ƒæ—¥æœŸ: {metadata['published']}")

    # æå–æ­£æ–‡å†…å®¹
    wechat_selector = "#js_content"
    content_element = soup.select_one(wechat_selector)
    if content_element:
        print(f"   âœ… æˆåŠŸåŒ¹é…åˆ°å¾®ä¿¡æ­£æ–‡å†…å®¹: '{wechat_selector}'")
    else:
        print(f"   âŒ æœªèƒ½é€šè¿‡ '{wechat_selector}' æ‰¾åˆ°å†…å®¹ã€‚")
    return content_element, metadata


def _process_generic_html(soup: BeautifulSoup, html_content: str) -> tuple[Tag | None, dict]:
    """å¤„ç†é€šç”¨ç½‘é¡µçš„HTMLï¼Œé€šè¿‡å¤šç§ç­–ç•¥æå–å…ƒæ•°æ®å’Œæ­£æ–‡ã€‚"""
    print("ğŸ¤– æœªåŒ¹é…åˆ°ç‰¹å®šè§„åˆ™ï¼Œå¯åŠ¨é€šç”¨å¤„ç†å™¨...")
    # é¦–å…ˆï¼Œå°è¯•ä» head ä¸­æå–é€šç”¨å…ƒæ•°æ®ä½œä¸ºåŸºç¡€
    metadata = _extract_head_metadata(soup)
    content_element = None

    # ç­–ç•¥1: å°è¯•é¢„è®¾çš„é€šç”¨é€‰æ‹©å™¨åˆ—è¡¨æ¥å®šä½æ­£æ–‡
    candidate_selectors = [
        'article', 'main', '#content', '#main-content', '#main',
        '.post-body', '.entry-content', '.article-body',
    ]
    for candidate in candidate_selectors:
        element = soup.select_one(candidate)
        if element:
            print(f"   âœ… é€šè¿‡é¢„è®¾è§„åˆ™æˆåŠŸåŒ¹é…åˆ°å†…å®¹: '{candidate}'")
            content_element = element
            # åœ¨é€šç”¨æ£€æµ‹æµç¨‹ä¸­ï¼Œå°è¯•è·å–é¡µé¢çš„ <title> ä½œä¸ºæ ‡é¢˜
            # ä»…å½“ metadata ä¸­è¿˜æ²¡æœ‰æ ‡é¢˜æ—¶ï¼Œæ‰ä½¿ç”¨ <title> æ ‡ç­¾ä½œä¸ºå¤‡ç”¨
            if not metadata.get("title") and soup.title and soup.title.string:
                metadata["title"] = soup.title.string.strip()
            break
    
    # ç­–ç•¥2: å¦‚æœé¢„è®¾è§„åˆ™å¤±è´¥ï¼Œåˆ™ä½¿ç”¨ Readability ç®—æ³•ä½œä¸ºæœ€ç»ˆçš„æ­£æ–‡æå–å°è¯•
    if not content_element:
        print("   - é¢„è®¾è§„åˆ™å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ Readability ç®—æ³•è¿›è¡Œæ™ºèƒ½æå–...")
        try:
            doc = Document(html_content)
            # Readability æå–çš„æ ‡é¢˜å’Œä½œè€…ä¼˜å…ˆçº§æ›´é«˜ï¼Œè¦†ç›–ä» head ä¸­è·å–çš„
            metadata["title"] = doc.title() 
            # ä½¿ç”¨ getattr() å®‰å…¨åœ°è®¿é—® byline å±æ€§ï¼Œå¦‚æœå±æ€§ä¸å­˜åœ¨åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œä»¥å¢å¼ºä»£ç å¥å£®æ€§å¹¶è§£å†³ Pylance è­¦å‘Š
            metadata["author"] = getattr(doc, 'byline', '') 
            main_content_html = doc.summary()
            content_element = BeautifulSoup(main_content_html, "html5lib")
            print("   âœ… Readability ç®—æ³•æˆåŠŸæå–åˆ°ä¸»è¦å†…å®¹ï¼")
        except Exception as e:
            print(f"   âŒ Readability ç®—æ³•æå–å¤±è´¥: {e}")

    return content_element, metadata


# --- 2-2. å†…å®¹åå¤„ç† ---
def _post_process_content(content_element: Tag, url: str):
    """å¯¹æå–å‡ºçš„å†…å®¹è¿›è¡Œåå¤„ç†ï¼Œä¸»è¦æ˜¯ä¿®æ­£å›¾ç‰‡URLã€‚"""
    # éå†æ‰€æœ‰ img æ ‡ç­¾
    for img in content_element.find_all('img'):
        # 1. å¤„ç†æ‡’åŠ è½½å±æ€§ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ data-src æˆ– data-original ç­‰å±æ€§ï¼Œå¹¶å°†å…¶å€¼èµ‹ç»™ src
        # å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é€šå¸¸ä½¿ç”¨ data-src æ¥å­˜å‚¨çœŸå®çš„å›¾ç‰‡ URL
        if 'data-src' in img.attrs:
            # å¦‚æœ data-src å­˜åœ¨ï¼Œå°±ç”¨å®ƒçš„å€¼æ¥æ›´æ–° src å±æ€§
            img['src'] = img['data-src']
            # ç§»é™¤ data-src å±æ€§ï¼Œé¿å…å†—ä½™ï¼Œå¹¶ä¸”è®© HTML æ›´â€œå¹²å‡€â€
            del img['data-src'] 
        elif 'data-original' in img.attrs: # æŸäº›ç½‘ç«™å¯èƒ½ä½¿ç”¨ data-original
            img['src'] = img['data-original']
            del img['data-original']
        # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ å…¶ä»–å¸¸è§çš„æ‡’åŠ è½½å±æ€§ï¼Œä¾‹å¦‚ _src ç­‰

        # 2. ç¡®ä¿ src å±æ€§æ˜¯ç»å¯¹è·¯å¾„
        # åªæœ‰å½“ src å±æ€§å­˜åœ¨ä¸”ä¸æ˜¯ç»å¯¹è·¯å¾„ï¼ˆå³ä¸ä»¥ http:// æˆ– https:// å¼€å¤´ï¼‰æ—¶æ‰è¿›è¡Œå¤„ç†
        if 'src' in img.attrs:
            # å…ˆå°†å±æ€§å€¼å–å‡ºï¼Œå¹¶ä½¿ç”¨ str() ç¡®ä¿å…¶ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼Œä»¥é˜²æ­¢ BeautifulSoup è¿”å›åˆ—è¡¨æˆ–å…¶ä»–ç±»å‹å¯¼è‡´é”™è¯¯
            src_value = str(img['src'])
            # å¯¹è½¬æ¢åçš„å­—ç¬¦ä¸²è¿›è¡Œåˆ¤æ–­å’Œå¤„ç†
            if not src_value.startswith(('http://', 'https://')):
            # ä½¿ç”¨ urljoin å°†ç›¸å¯¹è·¯å¾„ä¸é¡µé¢çš„åŸºç¡€ URL ç»„åˆï¼Œç”Ÿæˆç»å¯¹è·¯å¾„
            # urljoin èƒ½å¤Ÿæ™ºèƒ½å¤„ç†å„ç§ç›¸å¯¹è·¯å¾„æƒ…å†µ
                img['src'] = urljoin(url, src_value)
            # print(f"   ğŸ’¡ ä¿®æ­£å›¾ç‰‡URL: {img['src']}") # è°ƒè¯•ç”¨ï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡ŠæŸ¥çœ‹ä¿®æ­£è¿‡ç¨‹


# --- 2-3. è°ƒåº¦ä¸è½¬æ¢ ---
def convert_html_to_markdown(html_content: str, url: str) -> tuple[str, dict] | None:
    """
    ä» HTML å­—ç¬¦ä¸²ä¸­æå–ç‰¹å®šå†…å®¹å¹¶è½¬æ¢ä¸º Markdownã€‚è¿™æ˜¯ä¸€ä¸ªè°ƒåº¦å‡½æ•°ã€‚
    :param html_content: åŒ…å«å®Œæ•´ç½‘é¡µçš„ HTML å­—ç¬¦ä¸²ã€‚
    :param url: åŸå§‹ç½‘é¡µçš„ URLï¼Œç”¨äºå¹³å°ç‰¹å®šè§„åˆ™çš„åˆ¤æ–­ã€‚
    :return: æˆåŠŸæ—¶è¿”å›ä¸€ä¸ªåŒ…å« (Markdown å­—ç¬¦ä¸², å…ƒæ•°æ®å­—å…¸) çš„å…ƒç»„ï¼Œå¤±è´¥æ—¶è¿”å› Noneã€‚
    """
    print("\nğŸ” å¼€å§‹è§£æå†…å®¹...")
    # BeautifulSoup æ¥æ”¶å‰é¢ç”Ÿæˆçš„ç½‘é¡µå­—ç¬¦ä¸²ï¼Œè§£æç”Ÿæˆå†…éƒ¨çš„æ ‘çŠ¶æ•°æ®ç»“æ„
    # â€œhtml.parserâ€æ˜¯è§£æå™¨ï¼Œè¿˜æœ‰lxmlã€html5lib
    # è¿™ä¸ª soup å¯¹è±¡ç°åœ¨æ˜¯æ•´ä¸ª HTML æ–‡æ¡£çš„ Pythonic è¡¨ç¤º,ã€‚
    # ä½ å¯ä»¥æŠŠå®ƒçœ‹ä½œä¸€ä¸ªå¤æ‚çš„ã€åµŒå¥—çš„ Python å¯¹è±¡ï¼Œå®ƒå®Œæ•´åœ°æ˜ å°„äº†åŸå§‹ HTML çš„æ ‡ç­¾ã€å±æ€§å’Œæ–‡æœ¬å†…å®¹ã€‚
    soup = BeautifulSoup(html_content, "html5lib")

    content_element = None
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å…ƒæ•°æ®å­—å…¸
    metadata = {}

    # --- è°ƒåº¦ä¸­å¿ƒ ---
    # æ ¹æ® URL åˆ¤æ–­åº”è¯¥ä½¿ç”¨å“ªä¸ªå¤„ç†å™¨
    if "mp.weixin.qq.com" in url:
        content_element, metadata = _process_wechat_html(soup)
    else:
        content_element, metadata = _process_generic_html(soup, html_content)
    
    # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½æœªèƒ½æ‰¾åˆ°å†…å®¹ï¼Œåˆ™é€€å‡º
    if not content_element:
        print("âŒ æ‰€æœ‰è‡ªåŠ¨æ£€æµ‹æ–¹æ³•å‡å¤±è´¥ï¼Œæœªèƒ½æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸã€‚")
        return None
    
    print("âœ… æˆåŠŸæ‰¾åˆ°å†…å®¹å…ƒç´ ")
    
    # å¯¹æå–å‡ºçš„å†…å®¹è¿›è¡Œåå¤„ç†ï¼ˆä¾‹å¦‚ä¿®æ­£å›¾ç‰‡é“¾æ¥ï¼‰
    _post_process_content(content_element, url)

    # å°† HTML å…ƒç´ è½¬æ¢ä¸º Markdownæ–‡æ¡£å­—ç¬¦ä¸²ï¼Œmarkdownify()å‡½æ•°
    # heading_styleæ§åˆ¶ markdownify åœ¨å°† HTML æ ‡é¢˜æ ‡ç­¾ï¼ˆå¦‚ <h1>, <h2>, <h3> ç­‰ï¼‰è½¬æ¢ä¸º Markdown æ ‡é¢˜æ—¶æ‰€ä½¿ç”¨çš„æ ·å¼ã€‚
    # strip=['a'] å‚æ•°å¯ä»¥åœ¨è½¬æ¢å‰ç§»é™¤æ‰€æœ‰<a>æ ‡ç­¾ï¼Œä»¥è·å¾—æ›´å¹²å‡€çš„æ–‡æœ¬ã€‚
    markdown_text = markdownify(str(content_element), heading_style="ATX", strip=['a'])
    print(f"ğŸ”„ å·²å°† HTML (æ ‡é¢˜: {metadata.get('title', 'N/A')}) è½¬æ¢ä¸º Markdown")
    return markdown_text, metadata

def _create_front_matter(metadata: dict, url: str) -> str:
    """æ ¹æ®æå–çš„å…ƒæ•°æ®ç”Ÿæˆ YAML Front Matter å­—ç¬¦ä¸²ã€‚"""
    # ä½¿ç”¨ isoformat() è·å–ç¬¦åˆ ISO 8601 æ ‡å‡†çš„æ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²
    created_time = datetime.now().isoformat()

    # æ„å»º Front Matter å­—å…¸
    front_matter_data = {
        "note_type": "æ–‡çŒ®ç¬”è®°",
        "content_type": "æ–°é—»æŠ¥é“",
        "created": created_time,
        "published": metadata.get("published", ""),
        "source": url,
        "author": metadata.get("author", ""),
        "description": metadata.get("description", ""),
        "site_name": metadata.get("site_name", ""),
    }

    # å°†å­—å…¸æ ¼å¼åŒ–ä¸º YAML å­—ç¬¦ä¸²
    yaml_lines = ["---"]
    for key, value in front_matter_data.items():
        yaml_lines.append(f"{key}: {value}")
    yaml_lines.append("---")

    return "\n".join(yaml_lines)


# --- 3. ä¿å­˜åˆ°æ–‡ä»¶ ---
def save_to_file(content: str, user_specified_path: str | None, page_title: str):
    """
    å°†å­—ç¬¦ä¸²å†…å®¹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶ä¸­ã€‚
    :param content: è¦ä¿å­˜çš„å­—ç¬¦ä¸²å†…å®¹ã€‚
    :param user_specified_path: ç”¨æˆ·é€šè¿‡å‘½ä»¤è¡ŒæŒ‡å®šçš„è¾“å‡ºè·¯å¾„ï¼Œå¯èƒ½ä¸º Noneã€‚
    :param page_title: ä»ç½‘é¡µä¸­æå–çš„æ ‡é¢˜ï¼Œç”¨äºåœ¨ç”¨æˆ·æœªæŒ‡å®šè·¯å¾„æ—¶ç”Ÿæˆæ–‡ä»¶åã€‚
    """
    try:
        # å°†å‡€åŒ–æ–‡ä»¶åçš„é€»è¾‘å†…è”åˆ°è¿™é‡Œï¼šç§»é™¤æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦ï¼Œç„¶åæ·»åŠ  .md åç¼€
        sanitized_title_filename = re.sub(r'[\\/*?:"<>|]', "", str(page_title)).strip() + ".md"

        if user_specified_path: # ç”¨æˆ·æŒ‡å®šäº† -o å‚æ•°
            # åˆ¤æ–­ç”¨æˆ·æŒ‡å®šçš„æ˜¯ä¸€ä¸ªç›®å½•è¿˜æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
            if os.path.isdir(user_specified_path) or user_specified_path.endswith(('/', '\\')):
                # å¦‚æœç”¨æˆ·æŒ‡å®šçš„æ˜¯ä¸€ä¸ªç›®å½•ï¼Œåˆ™å°†æ ‡é¢˜ä½œä¸ºæ–‡ä»¶åä¸ç›®å½•ç»„åˆ
                output_path = os.path.join(user_specified_path, sanitized_title_filename)
            else:
                # å¦‚æœç”¨æˆ·æŒ‡å®šçš„æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«æ–‡ä»¶åï¼‰ï¼Œåˆ™ç›´æ¥ä½¿ç”¨
                output_path = user_specified_path
        else:
            # ç”¨æˆ·æœªæŒ‡å®š -o å‚æ•°ï¼Œåˆ™åœ¨å½“å‰ç›®å½•ä½¿ç”¨æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
            output_path = sanitized_title_filename

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)

        # ä½¿ç”¨ with open() è¯­å¥ç¡®ä¿æ–‡ä»¶æ“ä½œçš„å®‰å…¨æ€§å’Œèµ„æºçš„è‡ªåŠ¨é‡Šæ”¾
        with open(output_path, "w", encoding="utf-8") as f: 
            f.write(content)
        print(f"ğŸ’¾ æ–‡ä»¶å·²æˆåŠŸä¿å­˜åˆ°: {os.path.abspath(output_path)}") # ä½¿ç”¨ os.path.abspath è·å–ç»å¯¹è·¯å¾„ï¼Œè®©è¾“å‡ºæ›´æ˜ç¡®
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")

def _extract_urls_from_file(file_path: str) -> list[str]:
    """
    ä»ç»™å®šçš„æ–‡ä»¶ä¸­è¯»å–å†…å®¹ï¼Œå¹¶ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰ URLã€‚
    :param file_path: åŒ…å« URL çš„æ–‡ä»¶è·¯å¾„ã€‚
    :return: ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ‰¾åˆ°çš„ URL çš„åˆ—è¡¨ã€‚
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰ http/https é“¾æ¥
        urls = re.findall(r'https?://[^\s<>"]+|www\.[^\s<>"]+', content)
        # ä½¿ç”¨ set å»é‡ï¼Œç„¶åè½¬å› list
        unique_urls = list(set(urls))
        print(f"ğŸ“„ ä»æ–‡ä»¶ '{file_path}' ä¸­æ‰¾åˆ° {len(unique_urls)} ä¸ªå”¯ä¸€é“¾æ¥ã€‚")
        return unique_urls
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return []
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return []

# --- 4. ä¸»æµç¨‹ ---

async def main():
    """
    ç¨‹åºçš„ä¸»å¼‚æ­¥å…¥å£ï¼Œè´Ÿè´£ç¼–æ’æ•´ä¸ªæŠ“å–ã€è½¬æ¢å’Œä¿å­˜çš„å·¥ä½œæµã€‚
    """
    # ä½¿ç”¨ argparse è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="ä¸€ä¸ªé€šç”¨çš„ç½‘é¡µå†…å®¹æŠ“å–å¹¶è½¬æ¢ä¸º Markdown çš„å·¥å…·ã€‚")
    parser.add_argument("input_source", help="è¦æŠ“å–çš„ç›®æ ‡ç½‘é¡µ URLï¼Œæˆ–åŒ…å«å¤šä¸ª URL çš„æ–‡ä»¶è·¯å¾„ã€‚") # ä½ç½®å‚æ•°ï¼Œå¿…éœ€
    # ä¿®æ”¹-oå‚æ•°ï¼Œä½¿å…¶é»˜è®¤å€¼ä¸ºNoneï¼Œä»¥ä¾¿æˆ‘ä»¬åˆ¤æ–­ç”¨æˆ·æ˜¯å¦çœŸçš„è¾“å…¥äº†å®ƒ
    parser.add_argument("-o", "--output", help="è¾“å‡ºçš„ Markdown æ–‡ä»¶è·¯å¾„ã€‚å¦‚æœæœªæä¾›ï¼Œå°†æ ¹æ®ç½‘é¡µæ ‡é¢˜è‡ªåŠ¨ç”Ÿæˆã€‚")
    args = parser.parse_args()

    urls_to_process = []
    # åˆ¤æ–­è¾“å…¥æ˜¯æ–‡ä»¶è¿˜æ˜¯ URL
    if os.path.isfile(args.input_source):
        urls_to_process = _extract_urls_from_file(args.input_source)
    else:
        # å¦‚æœä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„ï¼Œåˆ™å‡å®šå®ƒæ˜¯ä¸€ä¸ª URL
        urls_to_process.append(args.input_source)

    for i, url in enumerate(urls_to_process):
        print(f"\n--- æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{len(urls_to_process)} ä¸ªé“¾æ¥: {url} ---")
        # 1. æå–
        html_content = await fetch_html_from_url(url)
        if not html_content:
            continue # å¦‚æœå½“å‰ URL å¤±è´¥ï¼Œåˆ™è·³è¿‡å¹¶ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª

        # 2. è½¬æ¢
        conversion_result = convert_html_to_markdown(html_content, url)
        if not conversion_result:
            continue
        
        markdown_text, metadata = conversion_result

        # 2.5. ç”Ÿæˆ Front Matter
        front_matter = _create_front_matter(metadata, url)

        # å°† Front Matter å’Œæ­£æ–‡å†…å®¹æ‹¼æ¥èµ·æ¥
    final_content = f"{front_matter}{SUMMARY_TEMPLATE}{markdown_text}"

        # 3. ä¿å­˜
    save_to_file(final_content, args.output, metadata.get("title", "Untitled"))

# --- 5. ç¨‹åºä¸»å…¥å£ ---

if __name__ == "__main__":
    # å› ä¸ºæˆ‘ä»¬çš„æ ¸å¿ƒå‡½æ•°æ˜¯å¼‚æ­¥çš„ï¼Œæ‰€ä»¥éœ€è¦ä½¿ç”¨ asyncio.run() æ¥å¯åŠ¨å®ƒ
    asyncio.run(main())
