
# å¯¼å…¥æˆ‘ä»¬éœ€è¦çš„åº“
import asyncio  # å¯¼å…¥ asyncio åº“ï¼Œå› ä¸º Playwright æ˜¯åŸºäºå¼‚æ­¥ I/O çš„ï¼Œéœ€è¦å®ƒæ¥è¿è¡Œ
import argparse # å¯¼å…¥ argparse åº“ä»¥å¤„ç†å‘½ä»¤è¡Œå‚æ•°
from playwright.async_api import async_playwright # ä» playwright åº“ä¸­å¯¼å…¥å¼‚æ­¥ API
from bs4 import BeautifulSoup # å¯¼å…¥ BeautifulSoup ç”¨äºè§£æ HTML
from markdownify import markdownify # å¯¼å…¥ markdownify ç”¨äºå°† HTML è½¬ä¸º Markdown
import os # å¯¼å…¥ os åº“ï¼Œç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„
import re # å¯¼å…¥ re åº“ï¼Œç”¨äºæ­£åˆ™è¡¨è¾¾å¼æ“ä½œï¼Œä»¥å‡€åŒ–æ–‡ä»¶å
from urllib.parse import urljoin # å¯¼å…¥ urljoin ç”¨äºå¤„ç†ç›¸å¯¹ URL è·¯å¾„

# --- 2. æŠ“å–HTMLå†…å®¹ ---

async def fetch_html_from_url(url: str) -> str | None:
    """
    ä½¿ç”¨ Playwright å¼‚æ­¥æŠ“å–æŒ‡å®š URL çš„ HTML å†…å®¹ã€‚
    :param url: ç›®æ ‡ç½‘é¡µçš„ URLã€‚
    :return: æˆåŠŸæ—¶è¿”å› HTML å­—ç¬¦ä¸²ï¼Œå¤±è´¥æ—¶è¿”å› Noneã€‚
    """
    print("ğŸš€ è„šæœ¬å¯åŠ¨ï¼Œå‡†å¤‡è¿æ¥ Playwright...")
    
    # é€šè¿‡async with è¯­å¥æ¥ç®¡ç† Playwright çš„ç”Ÿå‘½å‘¨æœŸï¼Œç¡®ä¿æµè§ˆå™¨è¢«æ­£ç¡®å…³é—­
    # asyncè¡¨ç¤ºä»£ç å¯ä»¥å¼‚æ­¥å¤„ç†ï¼Œæ‰€è°“å¼‚æ­¥ï¼Œæ˜¯æŒ‡å¯åœ¨æ‰§è¡Œä¸­æš‚åœï¼Œç­‰å¾…èµ„æºåˆ°ä½å†é‡å¯
    # withè¡¨ç¤ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œæ‰€è°“ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œæ˜¯æŒ‡è‡ªåŠ¨å¤„ç†ç›¸å…³åå°èµ„æºçš„å¼€å¯å’Œé‡Šæ”¾ï¼Œå¦‚æ–‡ä»¶ã€ç½‘ç»œè¿æ¥ç­‰
    # async_playwright()æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå¯åŠ¨ç›¸å…³èµ„æºï¼Œè¿”å›ä¸€ä¸ªPlaywright ä¸»æ§åˆ¶å¯¹è±¡ç»™p
    async with async_playwright() as p:
        try:
            # å¼‚æ­¥ï¼ˆå¯awaitï¼‰å¯åŠ¨ä¸€ä¸ª Chromium æµè§ˆå™¨å®ä¾‹ï¼ˆBrowserTypeå¯¹è±¡ï¼‰ã€‚headless=True è¡¨ç¤ºåœ¨åå°è¿è¡Œï¼Œä¸æ˜¾ç¤ºæµè§ˆå™¨çª—å£ã€‚
            # ä½ ä¹Ÿå¯ä»¥æ¢æˆ p.firefox.launch() æˆ– p.webkit.launch()
            # è¯¥æ–¹æ³•è¿”å›ä¸€ä¸ªBrowserå¯¹è±¡ï¼Œèµ‹å€¼ç»™browser
            browser = await p.chromium.launch(headless=True)
            print("âœ… æµè§ˆå™¨å·²å¯åŠ¨")

            # åœ¨æµè§ˆå™¨ä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„é¡µé¢ï¼ˆPageå¯¹è±¡ï¼‰ï¼Œå¹¶è®¾ç½®ä¸€ä¸ªçœŸå®çš„ User-Agent æ¥æ¨¡æ‹Ÿæ™®é€šç”¨æˆ·ï¼Œé˜²æ­¢åŸºç¡€çš„åçˆ¬è™«æ£€æµ‹ã€‚
            # user_agent æ˜¯ browser.new_page æ–¹æ³•çš„ä¸€ä¸ªå…³é”®å­—å‚æ•°ï¼ˆkeyword argumentï¼‰ã€‚
            page = await browser.new_page(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
            )
            print("âœ… é¡µé¢å·²åˆ›å»ºï¼Œå¹¶è®¾ç½®äº†è‡ªå®šä¹‰ User-Agent")
            print(f"ğŸŒ æ­£åœ¨å¯¼èˆªåˆ°: {url}")

            # è®¿é—®æˆ‘ä»¬æƒ³è¦æŠ“å–çš„ URLï¼Œå¹¶ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            # await å…³é”®å­—è¡¨ç¤ºâ€œç­‰å¾…è¿™ä¸ªæ“ä½œå®Œæˆå†ç»§ç»­â€
            # å¢åŠ  timeout å‚æ•°ï¼Œå°†é»˜è®¤çš„30ç§’è¶…æ—¶å»¶é•¿åˆ°60ç§’ (60000æ¯«ç§’)
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)
            print("âœ… é¡µé¢åŠ è½½å®Œæˆ")

            # è·å–å½“å‰é¡µé¢çš„å®Œæ•´ HTML å†…å®¹ï¼ˆä¹Ÿå°±æ˜¯html_contentï¼‰
            html_content = await page.content()
            print("ğŸ“„ å·²è·å–é¡µé¢ HTML å†…å®¹")

            # æ“ä½œå®Œæˆåï¼Œå…³é—­æµè§ˆå™¨
            await browser.close()
            print("âœ… æµè§ˆå™¨å·²å…³é—­")
            return html_content
        except Exception as e:
            print(f"âŒ åœ¨ä½¿ç”¨ Playwright æŠ“å–ç½‘é¡µæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None

# --- 2. è½¬åŒ–HTMLå†…å®¹ä¸ºMDæ–‡ä»¶ ---

def convert_html_to_markdown(html_content: str, url: str) -> tuple[str, str] | None:
    """
    ä» HTML å­—ç¬¦ä¸²ä¸­æå–ç‰¹å®šå†…å®¹å¹¶è½¬æ¢ä¸º Markdownã€‚
    :param html_content: åŒ…å«å®Œæ•´ç½‘é¡µçš„ HTML å­—ç¬¦ä¸²ã€‚
    :param url: åŸå§‹ç½‘é¡µçš„ URLï¼Œç”¨äºå¹³å°ç‰¹å®šè§„åˆ™çš„åˆ¤æ–­ã€‚
    :return: æˆåŠŸæ—¶è¿”å›ä¸€ä¸ªåŒ…å« (Markdown å­—ç¬¦ä¸², é¡µé¢æ ‡é¢˜) çš„å…ƒç»„ï¼Œå¤±è´¥æ—¶è¿”å› Noneã€‚
    """
    
    print("\nğŸ” å¼€å§‹è§£æå†…å®¹...")
    # BeautifulSoup æ¥æ”¶å‰é¢ç”Ÿæˆçš„ç½‘é¡µå­—ç¬¦ä¸²ï¼Œè§£æç”Ÿæˆå†…éƒ¨çš„æ ‘çŠ¶æ•°æ®ç»“æ„
    # â€œhtml.parserâ€æ˜¯è§£æå™¨ï¼Œè¿˜æœ‰lxmlã€html5lib
    # è¿™ä¸ª soup å¯¹è±¡ç°åœ¨æ˜¯æ•´ä¸ª HTML æ–‡æ¡£çš„ Pythonic è¡¨ç¤º,ã€‚
    # ä½ å¯ä»¥æŠŠå®ƒçœ‹ä½œä¸€ä¸ªå¤æ‚çš„ã€åµŒå¥—çš„ Python å¯¹è±¡ï¼Œå®ƒå®Œæ•´åœ°æ˜ å°„äº†åŸå§‹ HTML çš„æ ‡ç­¾ã€å±æ€§å’Œæ–‡æœ¬å†…å®¹ã€‚
    soup = BeautifulSoup(html_content, "html5lib")

    # åˆå§‹åŒ–ç½‘é¡µæ ‡é¢˜å’Œå†…å®¹å…ƒç´ å˜é‡
    page_title = "Untitled" # é»˜è®¤æ ‡é¢˜
    # åˆå§‹åŒ–å†…å®¹å…ƒç´ å˜é‡
    content_element = None

    # æ­¥éª¤ 1: æ£€æŸ¥æ˜¯å¦æœ‰å¹³å°ç‰¹å®šè§„åˆ™ï¼ˆä¾‹å¦‚å¾®ä¿¡å…¬ä¼—å·ï¼‰
    if "mp.weixin.qq.com" in url:
        print("ğŸ’¡ æ£€æµ‹åˆ°å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œå°è¯•ä½¿ç”¨ä¸“ç”¨é€‰æ‹©å™¨ '#js_content'...")
        wechat_selector = "#js_content"
        content_element = soup.select_one(wechat_selector)
        if content_element:
            print(f"   âœ… æˆåŠŸåŒ¹é…åˆ°å†…å®¹: '{wechat_selector}'") # ä»ç„¶æ‰“å°åŒ¹é…åˆ°çš„é€‰æ‹©å™¨ï¼Œä½†ä¸å†è®°å½•åˆ°å˜é‡
        else:
            # å¦‚æœå¾®ä¿¡ä¸“ç”¨é€‰æ‹©å™¨ä¹Ÿå¤±è´¥äº†ï¼ˆè™½ç„¶ä¸å¤ªå¯èƒ½ï¼‰ï¼Œåˆ™æ‰“å°æç¤ºå¹¶ç»§ç»­è¿›è¡Œé€šç”¨æ£€æµ‹
            print(f"   âŒ æœªèƒ½é€šè¿‡ '{wechat_selector}' æ‰¾åˆ°å†…å®¹ï¼Œå°†ç»§ç»­é€šç”¨æ£€æµ‹...")

    # æ­¥éª¤ 2: å¦‚æœä»¥ä¸Šæ–¹æ³•éƒ½æœªæˆåŠŸï¼Œåˆ™å¯åŠ¨é€šç”¨çš„é¢„è®¾åˆ—è¡¨è¿›è¡Œè‡ªåŠ¨æ£€æµ‹
    if not content_element:
        print("ğŸ¤– å¯åŠ¨é€šç”¨é¢„è®¾è§„åˆ™è¿›è¡Œæ£€æµ‹...")
        # å®šä¹‰ä¸€ä¸ªé«˜è´¨é‡çš„å€™é€‰é€‰æ‹©å™¨åˆ—è¡¨ï¼ŒæŒ‰å¯èƒ½æ€§ä»é«˜åˆ°ä½æ’åº
        candidate_selectors = [
            'article', 'main', '#content', '#main-content', '#main',
            '.post-body', '.entry-content', '.article-body',
        ]
        for candidate in candidate_selectors:
            # åœ¨é€šç”¨æ£€æµ‹æµç¨‹ä¸­ï¼Œå°è¯•è·å–é¡µé¢çš„ <title> ä½œä¸ºæ ‡é¢˜
            if soup.title and soup.title.string:
                page_title = soup.title.string.strip()

            print(f"   å°è¯•å€™é€‰é€‰æ‹©å™¨: '{candidate}'...")
            content_element = soup.select_one(candidate)
            if content_element: # ä»ç„¶æ‰“å°åŒ¹é…åˆ°çš„é€‰æ‹©å™¨ï¼Œä½†ä¸å†è®°å½•åˆ°å˜é‡
                print(f"   âœ… æˆåŠŸåŒ¹é…åˆ°å†…å®¹: '{candidate}'") 
                break # æ‰¾åˆ°åç«‹å³è·³å‡ºå¾ªç¯
    
    # æ­¥éª¤ 3: å¦‚æœæ‰€æœ‰è‡ªåŠ¨æ£€æµ‹éƒ½å¤±è´¥ï¼Œåˆ™æç¤ºå¹¶é€€å‡º
    if not content_element:
        print("âŒ è‡ªåŠ¨æ£€æµ‹å¤±è´¥ï¼Œæœªèƒ½æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸã€‚")
        return None

    print("âœ… æˆåŠŸæ‰¾åˆ°å†…å®¹å…ƒç´ ") # ä¸å†æ˜¾ç¤ºåŒ¹é…çš„é€‰æ‹©å™¨

    # --- æ–°å¢ï¼šå¤„ç†å›¾ç‰‡URLï¼Œç¡®ä¿å®ƒä»¬æ˜¯ç»å¯¹è·¯å¾„å¹¶å¤„ç†æ‡’åŠ è½½ ---
    # éå†æ‰€æœ‰ img æ ‡ç­¾
    for img in content_element.find_all('img'):
        # 1. å¤„ç†æ‡’åŠ è½½å±æ€§ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ data-src æˆ– data-original ç­‰å±æ€§ï¼Œå¹¶å°†å…¶å€¼èµ‹ç»™ src
        # å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é€šå¸¸ä½¿ç”¨ data-src æ¥å­˜å‚¨çœŸå®çš„å›¾ç‰‡ URL
        if 'data-src' in img.attrs:
            # å¦‚æœ data-src å­˜åœ¨ï¼Œå°±ç”¨å®ƒçš„å€¼æ¥æ›´æ–° src å±æ€§
            img['src'] = img['data-src']
            # ç§»é™¤ data-src å±æ€§ï¼Œé¿å…å†—ä½™ï¼Œå¹¶ä¸”è®© HTML æ›´â€œå¹²å‡€â€
            del img['data-src'] 
        elif 'data-original' in img.attrs: # æŸäº›ç½‘ç«™å¯èƒ½ä½¿ç”¨ data-original
            img['src'] = img['data-original']
            del img['data-original']
        # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ å…¶ä»–å¸¸è§çš„æ‡’åŠ è½½å±æ€§ï¼Œä¾‹å¦‚ _src ç­‰

        # 2. ç¡®ä¿ src å±æ€§æ˜¯ç»å¯¹è·¯å¾„
        # åªæœ‰å½“ src å±æ€§å­˜åœ¨ä¸”ä¸æ˜¯ç»å¯¹è·¯å¾„ï¼ˆå³ä¸ä»¥ http:// æˆ– https:// å¼€å¤´ï¼‰æ—¶æ‰è¿›è¡Œå¤„ç†
        if 'src' in img.attrs and not img['src'].startswith(('http://', 'https://')):
            # ä½¿ç”¨ urljoin å°†ç›¸å¯¹è·¯å¾„ä¸é¡µé¢çš„åŸºç¡€ URL ç»„åˆï¼Œç”Ÿæˆç»å¯¹è·¯å¾„
            # urljoin èƒ½å¤Ÿæ™ºèƒ½å¤„ç†å„ç§ç›¸å¯¹è·¯å¾„æƒ…å†µ
            img['src'] = urljoin(url, img['src'])
            # print(f"   ğŸ’¡ ä¿®æ­£å›¾ç‰‡URL: {img['src']}") # è°ƒè¯•ç”¨ï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡ŠæŸ¥çœ‹ä¿®æ­£è¿‡ç¨‹

    # --- ç»“æŸå›¾ç‰‡URLå¤„ç† ---

    # å°† HTML å…ƒç´ è½¬æ¢ä¸º Markdownæ–‡æ¡£å­—ç¬¦ä¸²ï¼Œmarkdownify()å‡½æ•°
    # heading_styleæ§åˆ¶ markdownify åœ¨å°† HTML æ ‡é¢˜æ ‡ç­¾ï¼ˆå¦‚ <h1>, <h2>, <h3> ç­‰ï¼‰è½¬æ¢ä¸º Markdown æ ‡é¢˜æ—¶æ‰€ä½¿ç”¨çš„æ ·å¼ã€‚
    # strip=['a'] å‚æ•°å¯ä»¥åœ¨è½¬æ¢å‰ç§»é™¤æ‰€æœ‰<a>æ ‡ç­¾ï¼Œä»¥è·å¾—æ›´å¹²å‡€çš„æ–‡æœ¬ã€‚
    markdown_text = markdownify(str(content_element), heading_style="ATX", strip=['a'])
    print(f"ğŸ”„ å·²å°† HTML (æ ‡é¢˜: {page_title}) è½¬æ¢ä¸º Markdown")
    return markdown_text, page_title

def save_to_file(content: str, user_specified_path: str | None, page_title: str):
    """
    å°†å­—ç¬¦ä¸²å†…å®¹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶ä¸­ã€‚
    :param content: è¦ä¿å­˜çš„å­—ç¬¦ä¸²å†…å®¹ã€‚
    :param user_specified_path: ç”¨æˆ·é€šè¿‡å‘½ä»¤è¡ŒæŒ‡å®šçš„è¾“å‡ºè·¯å¾„ï¼Œå¯èƒ½ä¸º Noneã€‚
    :param page_title: ä»ç½‘é¡µä¸­æå–çš„æ ‡é¢˜ï¼Œç”¨äºåœ¨ç”¨æˆ·æœªæŒ‡å®šè·¯å¾„æ—¶ç”Ÿæˆæ–‡ä»¶åã€‚
    """
    try:
        # å‡€åŒ–åçš„ç½‘é¡µæ ‡é¢˜ä½œä¸ºåŸºç¡€æ–‡ä»¶å
        sanitized_title_filename = sanitize_filename(page_title) + ".md"

        if user_specified_path: # ç”¨æˆ·æŒ‡å®šäº† -o å‚æ•°
            # åˆ¤æ–­ç”¨æˆ·æŒ‡å®šçš„æ˜¯ä¸€ä¸ªç›®å½•è¿˜æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
            if os.path.isdir(user_specified_path) or user_specified_path.endswith(('/', '\\')):
                # å¦‚æœç”¨æˆ·æŒ‡å®šçš„æ˜¯ä¸€ä¸ªç›®å½•ï¼Œåˆ™å°†æ ‡é¢˜ä½œä¸ºæ–‡ä»¶åä¸ç›®å½•ç»„åˆ
                output_path = os.path.join(user_specified_path, sanitized_title_filename)
            else:
                # å¦‚æœç”¨æˆ·æŒ‡å®šçš„æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«æ–‡ä»¶åï¼‰ï¼Œåˆ™ç›´æ¥ä½¿ç”¨
                output_path = user_specified_path
        else:
            # ç”¨æˆ·æœªæŒ‡å®š -o å‚æ•°ï¼Œåˆ™åœ¨å½“å‰ç›®å½•ä½¿ç”¨æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
            output_path = sanitized_title_filename

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)

        # ä½¿ç”¨ with open() è¯­å¥ç¡®ä¿æ–‡ä»¶æ“ä½œçš„å®‰å…¨æ€§å’Œèµ„æºçš„è‡ªåŠ¨é‡Šæ”¾
        with open(output_path, "w", encoding="utf-8") as f: 
            f.write(content)
        print(f"ğŸ’¾ æ–‡ä»¶å·²æˆåŠŸä¿å­˜åˆ°: {os.path.abspath(output_path)}") # ä½¿ç”¨ os.path.abspath è·å–ç»å¯¹è·¯å¾„ï¼Œè®©è¾“å‡ºæ›´æ˜ç¡®
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")

def sanitize_filename(filename: str) -> str:
    """
    ç§»é™¤æˆ–æ›¿æ¢æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦ã€‚
    :param filename: åŸå§‹æ–‡ä»¶åï¼ˆé€šå¸¸æ˜¯ç½‘é¡µæ ‡é¢˜ï¼‰ã€‚
    :return: æ¸…ç†åå¯ä»¥å®‰å…¨ç”¨ä½œæ–‡ä»¶åçš„å­—ç¬¦ä¸²ã€‚
    """
    # ç§»é™¤éæ³•å­—ç¬¦ï¼š \ / : * ? " < > |
    return re.sub(r'[\\/*?:"<>|]', "", filename).strip()

# --- 3. ä¸»æµç¨‹ç¼–æ’å‡½æ•° ---

async def main():
    """
    ç¨‹åºçš„ä¸»å¼‚æ­¥å…¥å£ï¼Œè´Ÿè´£ç¼–æ’æ•´ä¸ªæŠ“å–ã€è½¬æ¢å’Œä¿å­˜çš„å·¥ä½œæµã€‚
    """
    # ä½¿ç”¨ argparse è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="ä¸€ä¸ªé€šç”¨çš„ç½‘é¡µå†…å®¹æŠ“å–å¹¶è½¬æ¢ä¸º Markdown çš„å·¥å…·ã€‚")
    parser.add_argument("url", help="è¦æŠ“å–çš„ç›®æ ‡ç½‘é¡µ URLã€‚") # ä½ç½®å‚æ•°ï¼Œå¿…éœ€
    # ä¿®æ”¹-oå‚æ•°ï¼Œä½¿å…¶é»˜è®¤å€¼ä¸ºNoneï¼Œä»¥ä¾¿æˆ‘ä»¬åˆ¤æ–­ç”¨æˆ·æ˜¯å¦çœŸçš„è¾“å…¥äº†å®ƒ
    parser.add_argument("-o", "--output", help="è¾“å‡ºçš„ Markdown æ–‡ä»¶è·¯å¾„ã€‚å¦‚æœæœªæä¾›ï¼Œå°†æ ¹æ®ç½‘é¡µæ ‡é¢˜è‡ªåŠ¨ç”Ÿæˆã€‚")
    args = parser.parse_args()

    # 1. æå–
    html_content = await fetch_html_from_url(args.url)
    if not html_content:
        return

    # 2. è½¬æ¢
    conversion_result = convert_html_to_markdown(html_content, args.url) # ç°åœ¨åªä¼ é€’ HTML å†…å®¹å’Œ URL
    if not conversion_result:
        return
    
    markdown_text, page_title = conversion_result

    # 3. ä¿å­˜
    save_to_file(markdown_text, args.output, page_title)

# --- 4. ç¨‹åºä¸»å…¥å£ ---

if __name__ == "__main__":
    # å› ä¸ºæˆ‘ä»¬çš„æ ¸å¿ƒå‡½æ•°æ˜¯å¼‚æ­¥çš„ï¼Œæ‰€ä»¥éœ€è¦ä½¿ç”¨ asyncio.run() æ¥å¯åŠ¨å®ƒ
    asyncio.run(main())
