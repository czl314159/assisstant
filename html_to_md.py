"""
è„šæœ¬åç§°: HTML åˆ° Markdown è½¬æ¢å™¨ (html_to_md.py)

åŠŸèƒ½æè¿°:
    è¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ç½‘é¡µå†…å®¹æŠ“å–å’Œè½¬æ¢å·¥å…·ã€‚å®ƒèƒ½å¤Ÿä»æŒ‡å®šçš„ URL è·å–ç½‘é¡µå†…å®¹ï¼Œ
    æ™ºèƒ½æå–ä¸»è¦æ–‡ç« å†…å®¹ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå¹²å‡€çš„ Markdown æ ¼å¼ã€‚
    è½¬æ¢åçš„ Markdown æ–‡ä»¶ä¼šåŒ…å« YAML Front Matterï¼Œç”¨äºçŸ¥è¯†ç®¡ç†ï¼Œ
    å¹¶æ”¯æŒä¿å­˜å’Œé‡ç”¨ç™»å½•ä¼šè¯ï¼Œä»¥ä¾¿è®¿é—®éœ€è¦è®¤è¯çš„ç½‘ç«™ï¼ˆä¾‹å¦‚ã€Šåå°”è¡—æ—¥æŠ¥ã€‹ï¼‰ã€‚
    è„šæœ¬å¯ä»¥å¤„ç†å•ä¸ª URLï¼Œä¹Ÿå¯ä»¥ä»æ–‡ä»¶ä¸­è¯»å–å¤šä¸ª URL è¿›è¡Œæ‰¹é‡å¤„ç†ã€‚

ä½¿ç”¨æ–¹æ³•:
    1.  **è½¬æ¢å•ä¸ª URL**:
        `python html_to_md.py "https://example.com/article"`
        -   å¯é€‰å‚æ•° `-o` æˆ– `--output` æŒ‡å®šè¾“å‡ºç›®å½•æˆ–å®Œæ•´æ–‡ä»¶åã€‚
            ä¾‹å¦‚: `python html_to_md.py "https://example.com/article" -o /path/to/output/`
            æˆ– `python html_to_md.py "https://example.com/article" -o /path/to/output/my_article.md`

    2.  **ä»åŒ…å«å¤šä¸ª URL çš„æ–‡ä»¶è½¬æ¢**:
        `python html_to_md.py /path/to/your/links.txt`
        -   `links.txt` æ–‡ä»¶ä¸­æ¯è¡Œä¸€ä¸ª URLã€‚

    3.  **å¯åŠ¨äº¤äº’å¼ç™»å½•æµç¨‹å¹¶ä¿å­˜ä¼šè¯çŠ¶æ€**:
        `python html_to_md.py --login wsj`
        -   è¿™ä¼šæ‰“å¼€ä¸€ä¸ªæµè§ˆå™¨çª—å£ï¼Œè®©ä½ æ‰‹åŠ¨ç™»å½•ã€‚ç™»å½•æˆåŠŸåï¼Œä¼šè¯çŠ¶æ€å°†ä¿å­˜åˆ° `.env` ä¸­é…ç½®çš„è·¯å¾„ã€‚

é…ç½®:
    -   **WSJ ç™»å½•çŠ¶æ€è·¯å¾„**: å¦‚æœéœ€è¦ä¿å­˜ã€Šåå°”è¡—æ—¥æŠ¥ã€‹çš„ç™»å½•çŠ¶æ€ï¼Œè¯·åœ¨ `.env` æ–‡ä»¶ä¸­è®¾ç½®
        `WSJ_AUTH_STATE_PATH` ç¯å¢ƒå˜é‡ï¼Œä¾‹å¦‚: `WSJ_AUTH_STATE_PATH="/path/to/your/wsj_auth_state.json"`

ä¾èµ–:
    -   `playwright` (éœ€è¦é¢å¤–è¿è¡Œ `playwright install` å®‰è£…æµè§ˆå™¨é©±åŠ¨)
    -   `beautifulsoup4`
    -   `markdownify`
    -   `readability-lxml`
    -   `html5lib`
    -   `python-dotenv`
    -   `asyncio`
    -   `argparse`
    -   `os`, `re`, `datetime`, `json`, `random`, `urllib.parse` (Python å†…ç½®åº“)
"""


# å¯¼å…¥æˆ‘ä»¬éœ€è¦çš„åº“
import asyncio  # å¯¼å…¥ asyncio åº“ï¼Œå› ä¸º Playwright æ˜¯åŸºäºå¼‚æ­¥ I/O çš„ï¼Œéœ€è¦å®ƒæ¥è¿è¡Œ
import argparse # å¯¼å…¥ argparse åº“ä»¥å¤„ç†å‘½ä»¤è¡Œå‚æ•°
from playwright.async_api import async_playwright # ä» playwright åº“ä¸­å¯¼å…¥å¼‚æ­¥ API
from bs4 import BeautifulSoup # å¯¼å…¥ BeautifulSoup ç”¨äºè§£æ HTML
from markdownify import markdownify # å¯¼å…¥ markdownify ç”¨äºå°† HTML è½¬ä¸º Markdown
import os # å¯¼å…¥ os åº“ï¼Œç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„
import re # å¯¼å…¥ re åº“ï¼Œç”¨äºæ­£åˆ™è¡¨è¾¾å¼æ“ä½œï¼Œä»¥å‡€åŒ–æ–‡ä»¶å
from bs4.element import Tag # å¯¼å…¥ Tag ç±»å‹ç”¨äºç±»å‹æç¤º
from datetime import datetime # å¯¼å…¥ datetime ç”¨äºè·å–å½“å‰æ—¶é—´
import json # å¯¼å…¥ json åº“ï¼Œç”¨äºè§£æ JSON-LD æ•°æ®
import random # å¯¼å…¥ random åº“ï¼Œç”¨äºç”Ÿæˆéšæœºç­‰å¾…æ—¶é—´
from readability import Document # å¯¼å…¥ readability åº“ï¼Œç”¨äºæ™ºèƒ½æå–æ–‡ç« æ­£æ–‡
from urllib.parse import urljoin # å¯¼å…¥ urljoin ç”¨äºå¤„ç†ç›¸å¯¹ URL è·¯å¾„

from dotenv import load_dotenv

load_dotenv() # åœ¨æ‰€æœ‰ä»£ç ä¹‹å‰ï¼Œè¿è¡Œè¿™ä¸ªå‡½æ•°ï¼Œå®ƒä¼šè‡ªåŠ¨åŠ è½½.envæ–‡ä»¶

# --- å…¨å±€å¸¸é‡ ---
# å®šä¹‰ä¸€ä¸ªå¸¸é‡å­—ç¬¦ä¸²ï¼Œç”¨äºåœ¨ Front Matter ä¹‹åã€æ­£æ–‡ä¹‹å‰æ’å…¥çš„æ€»ç»“æç‚¼æ¨¡æ¿
SUMMARY_TEMPLATE = "\n# æ€»ç»“æç‚¼\n\n\n\n---\n\n"


# --- 1. é…ç½®æµè§ˆå™¨ä¸Šä¸‹æ–‡ ---
async def _setup_browser_context(browser, url):
    """
    æ ¹æ® URL é…ç½®å¹¶è¿”å›ä¸€ä¸ªåˆé€‚çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼ˆBrowserContextï¼‰ã€‚
    ç›®å‰ä¸»è¦ç”¨äºä¸ºç‰¹å®šç½‘ç«™ï¼ˆå¦‚åå°”è¡—æ—¥æŠ¥ï¼‰åŠ è½½ç™»å½• Cookiesã€‚
    :param browser: å½“å‰çš„ Playwright æµè§ˆå™¨å®ä¾‹ã€‚
    :param url: ç›®æ ‡ç½‘é¡µçš„ URLã€‚
    :return: ä¸€ä¸ªé…ç½®å¥½çš„ã€å…¨æ–°çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡å¯¹è±¡ã€‚
    """
    # å®šä¹‰ä¸€ä¸ªæ ‡å‡†çš„ã€çœŸå®çš„ User-Agent
    user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
    # å®šä¹‰ä¸€ä¸ªå¸¸è§çš„æµè§ˆå™¨è§†å£å¤§å°å’Œè¯­è¨€ï¼Œä»¥å®Œå–„æµè§ˆå™¨æŒ‡çº¹
    viewport = {'width': 1920, 'height': 1080}
    locale = 'en-US'
    
    # æ£€æŸ¥æ˜¯å¦ä¸º wsj.com çš„é“¾æ¥ï¼Œå¹¶å°è¯•åŠ è½½å·²ä¿å­˜çš„ä¼šè¯çŠ¶æ€
    if "wsj.com" in url:
        auth_state_path = os.environ.get('WSJ_AUTH_STATE_PATH')
        if auth_state_path and os.path.exists(auth_state_path):
            print(f"ğŸ’¡ æ£€æµ‹åˆ° WSJ é“¾æ¥ï¼Œæ­£åœ¨ä» '{auth_state_path}' åŠ è½½ä¼šè¯çŠ¶æ€...")
            try:
                # åŸºäºå·²ä¿å­˜çš„ä¼šè¯çŠ¶æ€æ–‡ä»¶åˆ›å»ºä¸Šä¸‹æ–‡ï¼Œè¿™å°†æ¢å¤ç™»å½•çŠ¶æ€
                context = await browser.new_context(
                    user_agent=user_agent,
                    storage_state=auth_state_path,
                    viewport=viewport,
                    locale=locale
                )
                print("   âœ… ä¼šè¯çŠ¶æ€åŠ è½½æˆåŠŸï¼")
                return context
            except Exception as e:
                print(f"   âŒ åŠ è½½ä¼šè¯çŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {e}ã€‚å°†ä»¥æœªç™»å½•çŠ¶æ€ç»§ç»­ã€‚")
    
    # å¦‚æœä¸åŒ¹é…ä»»ä½•ç‰¹æ®Šè§„åˆ™ï¼ˆWSJç½‘å€ï¼‰ï¼Œæˆ–åŠ è½½å¤±è´¥ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„ã€å¹²å‡€çš„ä¸Šä¸‹æ–‡
    print("   - æœªåŒ¹é…åˆ°ç‰¹æ®Šè§„åˆ™æˆ–åŠ è½½çŠ¶æ€å¤±è´¥ï¼Œåˆ›å»ºæ–°çš„å¹²å‡€ä¸Šä¸‹æ–‡ã€‚")
    context = await browser.new_context(
        user_agent=user_agent,
        viewport=viewport,
        locale=locale
    )
    return context

# --- 2. ä½¿ç”¨playwrightæŠ“å–HTMLå†…å®¹ ---
async def fetch_html_from_url(url: str) -> str | None:
    """
    ä½¿ç”¨ Playwright å¼‚æ­¥æŠ“å–æŒ‡å®š URL çš„ HTML å†…å®¹ã€‚
    :param url: ç›®æ ‡ç½‘é¡µçš„ URLã€‚
    :return: æˆåŠŸæ—¶è¿”å› HTML å­—ç¬¦ä¸²ï¼Œå¤±è´¥æ—¶è¿”å› Noneã€‚
    """
    print("ğŸš€ è„šæœ¬å¯åŠ¨ï¼Œå‡†å¤‡è¿æ¥ Playwright...")
    
    # é€šè¿‡async with è¯­å¥æ¥ç®¡ç† Playwright çš„ç”Ÿå‘½å‘¨æœŸï¼Œç¡®ä¿æµè§ˆå™¨è¢«æ­£ç¡®å…³é—­
    # asyncè¡¨ç¤ºä»£ç å¯ä»¥å¼‚æ­¥å¤„ç†ï¼Œæ‰€è°“å¼‚æ­¥ï¼Œæ˜¯æŒ‡å¯åœ¨æ‰§è¡Œä¸­æš‚åœï¼Œç­‰å¾…èµ„æºåˆ°ä½å†é‡å¯
    # withè¡¨ç¤ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œæ‰€è°“ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œæ˜¯æŒ‡è‡ªåŠ¨å¤„ç†ç›¸å…³åå°èµ„æºçš„å¼€å¯å’Œé‡Šæ”¾ï¼Œå¦‚æ–‡ä»¶ã€ç½‘ç»œè¿æ¥ç­‰
    # async_playwright()æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå¯åŠ¨ç›¸å…³èµ„æºï¼Œè¿”å›ä¸€ä¸ªPlaywright ä¸»æ§åˆ¶å¯¹è±¡ç»™p
    async with async_playwright() as p:
        try:
            # å¼‚æ­¥ï¼ˆå¯awaitï¼‰å¯åŠ¨ä¸€ä¸ª Chromium æµè§ˆå™¨å®ä¾‹ï¼ˆBrowserTypeå¯¹è±¡ï¼‰ã€‚headless=True è¡¨ç¤ºåœ¨åå°è¿è¡Œï¼Œä¸æ˜¾ç¤ºæµè§ˆå™¨çª—å£ã€‚
            # ä½ ä¹Ÿå¯ä»¥æ¢æˆ p.firefox.launch() æˆ– p.webkit.launch()
            # è¯¥æ–¹æ³•è¿”å›ä¸€ä¸ªBrowserå¯¹è±¡ï¼Œèµ‹å€¼ç»™browser
            browser = await p.chromium.launch(headless=True)
            print("âœ… æµè§ˆå™¨å·²å¯åŠ¨")
            
            # è°ƒç”¨è¾…åŠ©å‡½æ•°æ¥è·å–ä¸€ä¸ªé…ç½®å¥½çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡
            context = await _setup_browser_context(browser, url)

            # æˆ‘ä»¬ç›´æ¥ä»é…ç½®å¥½çš„ context åˆ›å»ºæ–°é¡µé¢
            page = await context.new_page()
            print(f"ğŸŒ æ­£åœ¨å¯¼èˆªåˆ°: {url}")

            # è®¿é—®æˆ‘ä»¬æƒ³è¦æŠ“å–çš„ URLï¼Œå¹¶ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            # await å…³é”®å­—è¡¨ç¤ºâ€œç­‰å¾…è¿™ä¸ªæ“ä½œå®Œæˆå†ç»§ç»­â€
            # å¢åŠ  timeout å‚æ•°ï¼Œå°†é»˜è®¤çš„30ç§’è¶…æ—¶å»¶é•¿åˆ°60ç§’ (60000æ¯«ç§’)
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)
            print("âœ… é¡µé¢åŠ è½½å®Œæˆ")

            # --- æ–°å¢ï¼šå¤„ç† Cookie åŒæ„å¼¹çª— ---
            print("ğŸ” æ­£åœ¨æ£€æŸ¥å¹¶å¤„ç† Cookie åŒæ„å¼¹çª—...")
            # å®šä¹‰ä¸€ä¸ªåŒ…å«å¤šç§å¸¸è§â€œåŒæ„â€æŒ‰é’®æ–‡æœ¬å’Œé€‰æ‹©å™¨çš„åˆ—è¡¨
            accept_selectors = [
                '#onetrust-accept-btn-handler',  # OneTrust å¹³å°çš„æ ‡å‡† ID
                'button:has-text("Accept All")',
                'button:has-text("I Accept")',
                'button:has-text("Agree")',
                'button:has-text("Accept")',
            ]
            # å°†æ‰€æœ‰é€‰æ‹©å™¨åˆå¹¶ä¸ºä¸€ä¸ªï¼ŒPlaywright ä¼šå°è¯•åŒ¹é…ç¬¬ä¸€ä¸ªå‡ºç°çš„å…ƒç´ 
            combined_selector = " , ".join(accept_selectors)
            try:
                # å°è¯•åœ¨ 5 ç§’å†…æ‰¾åˆ°å¹¶ç‚¹å‡»æŒ‰é’®ã€‚å¦‚æœæ‰¾ä¸åˆ°ï¼Œä¼šæŠ›å‡º TimeoutErrorã€‚
                await page.locator(combined_selector).first().click(timeout=5000)
                print("   âœ… å·²ç‚¹å‡» Cookie åŒæ„æŒ‰é’®ã€‚")
            except Exception:
                # å¦‚æœåœ¨è¶…æ—¶æ—¶é—´å†…æ‰¾ä¸åˆ°æŒ‰é’®ï¼Œæˆ–å‘ç”Ÿå…¶ä»–é”™è¯¯ï¼Œåˆ™é™é»˜å¤±è´¥å¹¶ç»§ç»­ã€‚
                # è¿™æ ·åšæ˜¯å®‰å…¨çš„ï¼Œå› ä¸ºå¤§å¤šæ•°æƒ…å†µä¸‹å¼¹çª—å¯èƒ½ä¸å­˜åœ¨ã€‚
                print("   - æœªæ‰¾åˆ°æˆ–æ— éœ€å¤„ç† Cookie åŒæ„å¼¹çª—ã€‚")

            # --- æ–°å¢ï¼šæ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸º ---
            print("ğŸš¶ æ­£åœ¨æ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸º...")
            # 1. æ¨¡æ‹Ÿé¡µé¢æ»šåŠ¨ï¼Œä»¥è§¦å‘æ‡’åŠ è½½å†…å®¹å¹¶ä½¿è¡Œä¸ºæ›´é€¼çœŸ
            await page.evaluate("""
                async () => {
                    const distance = 100; // æ¯æ¬¡æ»šåŠ¨çš„è·ç¦»
                    const delay = 100;    // æ¯æ¬¡æ»šåŠ¨åçš„å»¶è¿Ÿ

                    // --- æ–°å¢ï¼šå¥å£®çš„æ»šåŠ¨é€€å‡ºæœºåˆ¶ï¼Œé˜²æ­¢æ— é™å¾ªç¯ ---
                    const maxScrolls = 100;      // 1. è®¾ç½®æœ€å¤§æ»šåŠ¨æ¬¡æ•°ï¼Œä½œä¸ºç¡¬æ€§é€€å‡ºæ¡ä»¶
                    let scrolls = 0;
                    let lastScrollY = -1;
                    let stuckCount = 0;
                    const maxStuckCount = 5; // 2. å¦‚æœè¿ç»­5æ¬¡æ»šåŠ¨ä½ç½®ä¸å˜ï¼Œåˆ™è®¤ä¸ºåˆ°è¾¾åº•éƒ¨

                    while (scrolls < maxScrolls && stuckCount < maxStuckCount) {
                        const currentScrollY = window.scrollY;
                        window.scrollBy(0, distance);
                        await new Promise(resolve => setTimeout(resolve, delay));
                        scrolls++;

                        // æ£€æŸ¥æ»šåŠ¨ä½ç½®æ˜¯å¦å˜åŒ–
                        if (window.scrollY === currentScrollY) {
                            stuckCount++;
                        } else {
                            stuckCount = 0; // å¦‚æœæ»šåŠ¨äº†ï¼Œå°±é‡ç½®è®¡æ•°å™¨
                        }

                        // è¿™æ˜¯ä¸€ä¸ªé¢å¤–çš„ã€å¿«é€Ÿé€€å‡ºçš„æ¡ä»¶ï¼Œå¦‚æœå·²åˆ°è¾¾æ–‡æ¡£åº•éƒ¨åˆ™ç«‹å³åœæ­¢
                        if (window.scrollY + window.innerHeight >= document.body.scrollHeight) {
                            break;
                        }
                    }
                }
            """)
            print("   - é¡µé¢å·²æ»šåŠ¨åˆ°åº•éƒ¨ã€‚")
            # 2. åœ¨æŠ“å–å†…å®¹å‰éšæœºç­‰å¾…ä¸€æ®µæ—¶é—´
            short_wait = random.uniform(2, 5)
            print(f"   - éšæœºç­‰å¾… {short_wait:.2f} ç§’...")
            await asyncio.sleep(short_wait)

            # è·å–å½“å‰é¡µé¢çš„å®Œæ•´ HTML å†…å®¹ï¼ˆä¹Ÿå°±æ˜¯html_contentï¼‰
            html_content = await page.content()
            print("ğŸ“„ å·²è·å–é¡µé¢ HTML å†…å®¹")

            await context.close() # å…³é—­ä¸Šä¸‹æ–‡
            # æ“ä½œå®Œæˆåï¼Œå…³é—­æµè§ˆå™¨
            await browser.close()
            print("âœ… æµè§ˆå™¨å·²å…³é—­")
            return html_content
        except Exception as e:
            print(f"âŒ åœ¨ä½¿ç”¨ Playwright æŠ“å–ç½‘é¡µæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None


# --- 3. æå–é€šç”¨å…ƒæ•°æ® ---
def _extract_head_metadata(soup: BeautifulSoup) -> dict:
    """ä» HTML çš„ <head> éƒ¨åˆ†æå–é€šç”¨çš„å…ƒæ•°æ®ã€‚"""
    metadata = {}
    # å®šä¹‰å…ƒæ•°æ®æå–è§„åˆ™ï¼š(å…ƒæ•°æ®é”®å, CSSé€‰æ‹©å™¨, å±æ€§å)
    rules = [
        ('description', 'meta[name="description"]', 'content'),
        ('description', 'meta[property="og:description"]', 'content'),
        ('site_name', 'meta[property="og:site_name"]', 'content'),
    ]

    for key, selector, attr in rules:
        # å¦‚æœè¿™ä¸ªå…ƒæ•°æ®è¿˜æ²¡è¢«æ‰¾åˆ°ï¼Œå°±å°è¯•æŸ¥æ‰¾
        if key not in metadata or not metadata[key]:
            element = soup.select_one(selector)
            # å¢åŠ å¥å£®æ€§æ£€æŸ¥ï¼šç¡®ä¿ element å­˜åœ¨ï¼Œå¹¶ä¸”å¯¹åº”çš„å±æ€§ä¹Ÿå­˜åœ¨
            if element:
                value = element.get(attr)
                if value:
                    # ä½¿ç”¨ str() å°†è·å–åˆ°çš„å€¼ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰æ˜¾å¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç„¶åå†è°ƒç”¨ strip()
                    metadata[key] = str(value).strip()
    return metadata

# --- 4. å¤„ç†å¾®ä¿¡å…¬ä¼—å·HTML ---
def _process_wechat_html(soup: BeautifulSoup) -> tuple[Tag | None, dict]:
    """ä¸“é—¨å¤„ç†å¾®ä¿¡å…¬ä¼—å·æ–‡ç« çš„HTMLï¼Œæå–å…ƒæ•°æ®å’Œæ­£æ–‡ã€‚"""
    print("ğŸ’¡ æ£€æµ‹åˆ°å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œå¯åŠ¨ä¸“ç”¨å¤„ç†å™¨...")
    # é¦–å…ˆï¼Œå°è¯•ä» head ä¸­æå–é€šç”¨å…ƒæ•°æ®ä½œä¸ºåŸºç¡€
    metadata = _extract_head_metadata(soup)
    
    # ç„¶åï¼Œä½¿ç”¨å¾®ä¿¡ç‰¹å®šçš„é€‰æ‹©å™¨æ¥è¦†ç›–å’Œè¡¥å……å…ƒæ•°æ®ï¼Œå› ä¸ºå®ƒä»¬æ›´å‡†ç¡®
    title_element = soup.select_one('h1#activity-name')
    if title_element:
        metadata["title"] = title_element.get_text(strip=True)
        print(f"   âœ’ï¸ æˆåŠŸæå–å¾®ä¿¡æ ‡é¢˜: {metadata['title']}")

    author_element = soup.select_one('#js_name')
    if author_element:
        metadata["author"] = author_element.get_text(strip=True)
        print(f"   ğŸ‘¤ æˆåŠŸæå–å¾®ä¿¡ä½œè€…: {metadata['author']}")

    published_element = soup.select_one('em#publish_time')
    if published_element:
        metadata["published"] = published_element.get_text(strip=True)
        print(f"   ğŸ“… æˆåŠŸæå–å¾®ä¿¡å‘å¸ƒæ—¥æœŸ: {metadata['published']}")

    # æå–æ­£æ–‡å†…å®¹
    wechat_selector = "#js_content"
    content_element = soup.select_one(wechat_selector)
    if content_element:
        print(f"   âœ… æˆåŠŸåŒ¹é…åˆ°å¾®ä¿¡æ­£æ–‡å†…å®¹: '{wechat_selector}'")
    else:
        print(f"   âŒ æœªèƒ½é€šè¿‡ '{wechat_selector}' æ‰¾åˆ°å†…å®¹ã€‚")
    return content_element, metadata

# --- 5. å¤„ç†é€šç”¨HTML ---
def _process_generic_html(soup: BeautifulSoup, html_content: str) -> tuple[Tag | None, dict]:
    """å¤„ç†é€šç”¨ç½‘é¡µçš„HTMLï¼Œé€šè¿‡å¤šç§ç­–ç•¥æå–å…ƒæ•°æ®å’Œæ­£æ–‡ã€‚"""
    print("ğŸ¤– æœªåŒ¹é…åˆ°ç‰¹å®šè§„åˆ™ï¼Œå¯åŠ¨é€šç”¨å¤„ç†å™¨...")
    # é¦–å…ˆï¼Œå°è¯•ä» head ä¸­æå–é€šç”¨å…ƒæ•°æ®ä½œä¸ºåŸºç¡€
    metadata = _extract_head_metadata(soup)
    content_element = None

    # --- æ–°å¢ç­–ç•¥1: è§£æ JSON-LD ç»“æ„åŒ–æ•°æ® (æœ€é«˜ä¼˜å…ˆçº§) ---
    # è®¸å¤šç°ä»£ç½‘ç«™ä½¿ç”¨ JSON-LD æ¥æä¾›æœºå™¨å¯è¯»çš„å…ƒæ•°æ®ï¼Œè¿™é€šå¸¸æ˜¯æœ€å‡†ç¡®çš„ä¿¡æ¯æ¥æºã€‚
    json_ld_scripts = soup.find_all('script', type='application/ld+json')
    for script in json_ld_scripts:
        # æ£€æŸ¥ script æ ‡ç­¾å†…æ˜¯å¦æœ‰å†…å®¹ï¼Œå› ä¸º .string åœ¨æ ‡ç­¾ä¸ºç©ºæˆ–åŒ…å«å­æ ‡ç­¾æ—¶å¯èƒ½è¿”å› None
        if script.string:
            try:
                json_data = json.loads(script.string)

                # JSON-LD æ•°æ®å¯ä»¥æ˜¯å•ä¸ªå­—å…¸ï¼Œä¹Ÿå¯ä»¥æ˜¯å­—å…¸åˆ—è¡¨ã€‚æˆ‘ä»¬ç»Ÿä¸€å¤„ç†ã€‚
                items_to_process = []
                if isinstance(json_data, list):
                    items_to_process.extend(json_data)
                elif isinstance(json_data, dict):
                    items_to_process.append(json_data)

                # éå†æ‰€æœ‰æ‰¾åˆ°çš„ JSON-LD é¡¹ç›®
                for item in items_to_process:
                    if not isinstance(item, dict):
                        continue

                    # æŸ¥æ‰¾å¹¶æå–å‘å¸ƒæ—¥æœŸ
                    if not metadata.get("published") and item.get("datePublished"):
                        metadata["published"] = item["datePublished"]
                        print(f"   ğŸ“Š ä» JSON-LD æå–åˆ°å‘å¸ƒæ—¥æœŸ: {metadata['published']}")
                    
                    # æŸ¥æ‰¾å¹¶æå–ä½œè€…ä¿¡æ¯
                    if not metadata.get("author") and item.get("author"):
                        author_data = item["author"]
                        if isinstance(author_data, dict) and author_data.get("name"):
                            metadata["author"] = author_data["name"]
                        elif isinstance(author_data, list) and len(author_data) > 0 and author_data[0].get("name"):
                            metadata["author"] = author_data[0]["name"]
                        if metadata.get("author"):
                            print(f"   ğŸ“Š ä» JSON-LD æå–åˆ°ä½œè€…: {metadata['author']}")

            except (json.JSONDecodeError, TypeError):
                # å¦‚æœè„šæœ¬å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„ JSONï¼Œåˆ™é™é»˜å¤±è´¥å¹¶ç»§ç»­
                # TypeError ä¹Ÿä¼šè¢«æ•è·ï¼Œä»¥é˜²ä¸‡ä¸€ï¼Œå°½ç®¡æˆ‘ä»¬å·²ç»æ£€æŸ¥äº† script.string
                continue

    # ç­–ç•¥2: å°è¯•é¢„è®¾çš„é€šç”¨é€‰æ‹©å™¨åˆ—è¡¨æ¥å®šä½æ­£æ–‡
    candidate_selectors = [
        'article', 'main', '#content', '#main-content', '#main',
        '.post-body', '.entry-content', '.article-body',
    ]
    for candidate in candidate_selectors:
        element = soup.select_one(candidate)
        if element:
            print(f"   âœ… é€šè¿‡é¢„è®¾è§„åˆ™æˆåŠŸåŒ¹é…åˆ°å†…å®¹: '{candidate}'")
            content_element = element
            # åœ¨é€šç”¨æ£€æµ‹æµç¨‹ä¸­ï¼Œå°è¯•è·å–é¡µé¢çš„ <title> ä½œä¸ºæ ‡é¢˜
            # ä»…å½“ metadata ä¸­è¿˜æ²¡æœ‰æ ‡é¢˜æ—¶ï¼Œæ‰ä½¿ç”¨ <title> æ ‡ç­¾ä½œä¸ºå¤‡ç”¨
            if not metadata.get("title") and soup.title and soup.title.string:
                metadata["title"] = soup.title.string.strip()
            break
    
    # ç­–ç•¥3: å¦‚æœé¢„è®¾è§„åˆ™å¤±è´¥ï¼Œåˆ™ä½¿ç”¨ Readability ç®—æ³•ä½œä¸ºæœ€ç»ˆçš„æ­£æ–‡æå–å°è¯•
    if not content_element:
        print("   - é¢„è®¾è§„åˆ™å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ Readability ç®—æ³•è¿›è¡Œæ™ºèƒ½æå–...")
        try:
            doc = Document(html_content)
            # Readability æå–çš„æ ‡é¢˜å’Œä½œè€…ä¼˜å…ˆçº§æ›´é«˜ï¼Œè¦†ç›–ä» head ä¸­è·å–çš„
            if not metadata.get("title"):
                metadata["title"] = doc.title() 
            # ä»…å½“é€šè¿‡å…¶ä»–æ–¹å¼éƒ½æœªæ‰¾åˆ°ä½œè€…æ—¶ï¼Œæ‰ä½¿ç”¨ Readability çš„ byline ä½œä¸ºå¤‡ç”¨
            if not metadata.get("author"):
                metadata["author"] = getattr(doc, 'byline', '') 
            main_content_html = doc.summary()
            content_element = BeautifulSoup(main_content_html, "html5lib")
            print("   âœ… Readability ç®—æ³•æˆåŠŸæå–åˆ°ä¸»è¦å†…å®¹ï¼")
        except Exception as e:
            print(f"   âŒ Readability ç®—æ³•æå–å¤±è´¥: {e}")

    return content_element, metadata


# --- 6. å†…å®¹åå¤„ç† ---
def _post_process_content(content_element: Tag, url: str):
    """å¯¹æå–å‡ºçš„å†…å®¹è¿›è¡Œåå¤„ç†ï¼Œä¸»è¦æ˜¯ä¿®æ­£å›¾ç‰‡URLã€‚"""
    # éå†æ‰€æœ‰ img æ ‡ç­¾
    for img in content_element.find_all('img'):
        # 1. å¤„ç†æ‡’åŠ è½½å±æ€§ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ data-src æˆ– data-original ç­‰å±æ€§ï¼Œå¹¶å°†å…¶å€¼èµ‹ç»™ src
        # å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é€šå¸¸ä½¿ç”¨ data-src æ¥å­˜å‚¨çœŸå®çš„å›¾ç‰‡ URL
        if 'data-src' in img.attrs:
            # å¦‚æœ data-src å­˜åœ¨ï¼Œå°±ç”¨å®ƒçš„å€¼æ¥æ›´æ–° src å±æ€§
            img['src'] = img['data-src']
            # ç§»é™¤ data-src å±æ€§ï¼Œé¿å…å†—ä½™ï¼Œå¹¶ä¸”è®© HTML æ›´â€œå¹²å‡€â€
            del img['data-src'] 
        elif 'data-original' in img.attrs: # æŸäº›ç½‘ç«™å¯èƒ½ä½¿ç”¨ data-original
            img['src'] = img['data-original']
            del img['data-original']
        # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ å…¶ä»–å¸¸è§çš„æ‡’åŠ è½½å±æ€§ï¼Œä¾‹å¦‚ _src ç­‰

        # 2. ç¡®ä¿ src å±æ€§æ˜¯ç»å¯¹è·¯å¾„
        # åªæœ‰å½“ src å±æ€§å­˜åœ¨ä¸”ä¸æ˜¯ç»å¯¹è·¯å¾„ï¼ˆå³ä¸ä»¥ http:// æˆ– https:// å¼€å¤´ï¼‰æ—¶æ‰è¿›è¡Œå¤„ç†
        if 'src' in img.attrs:
            # å…ˆå°†å±æ€§å€¼å–å‡ºï¼Œå¹¶ä½¿ç”¨ str() ç¡®ä¿å…¶ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼Œä»¥é˜²æ­¢ BeautifulSoup è¿”å›åˆ—è¡¨æˆ–å…¶ä»–ç±»å‹å¯¼è‡´é”™è¯¯
            src_value = str(img['src'])
            # å¯¹è½¬æ¢åçš„å­—ç¬¦ä¸²è¿›è¡Œåˆ¤æ–­å’Œå¤„ç†
            if not src_value.startswith(('http://', 'https://')):
            # ä½¿ç”¨ urljoin å°†ç›¸å¯¹è·¯å¾„ä¸é¡µé¢çš„åŸºç¡€ URL ç»„åˆï¼Œç”Ÿæˆç»å¯¹è·¯å¾„
            # urljoin èƒ½å¤Ÿæ™ºèƒ½å¤„ç†å„ç§ç›¸å¯¹è·¯å¾„æƒ…å†µ
                img['src'] = urljoin(url, src_value)
            # print(f"   ğŸ’¡ ä¿®æ­£å›¾ç‰‡URL: {img['src']}") # è°ƒè¯•ç”¨ï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡ŠæŸ¥çœ‹ä¿®æ­£è¿‡ç¨‹


# --- 7. è°ƒåº¦HTMLè½¬æ¢ ---
def convert_html_to_markdown(html_content: str, url: str) -> tuple[str, dict] | None:
    """
    ä» HTML å­—ç¬¦ä¸²ä¸­æå–ç‰¹å®šå†…å®¹å¹¶è½¬æ¢ä¸º Markdownã€‚è¿™æ˜¯ä¸€ä¸ªè°ƒåº¦å‡½æ•°ã€‚
    :param html_content: åŒ…å«å®Œæ•´ç½‘é¡µçš„ HTML å­—ç¬¦ä¸²ã€‚
    :param url: åŸå§‹ç½‘é¡µçš„ URLï¼Œç”¨äºå¹³å°ç‰¹å®šè§„åˆ™çš„åˆ¤æ–­ã€‚
    :return: æˆåŠŸæ—¶è¿”å›ä¸€ä¸ªåŒ…å« (Markdown å­—ç¬¦ä¸², å…ƒæ•°æ®å­—å…¸) çš„å…ƒç»„ï¼Œå¤±è´¥æ—¶è¿”å› Noneã€‚
    """
    print("\nğŸ” å¼€å§‹è§£æå†…å®¹...")
    # BeautifulSoup æ¥æ”¶å‰é¢ç”Ÿæˆçš„ç½‘é¡µå­—ç¬¦ä¸²ï¼Œè§£æç”Ÿæˆå†…éƒ¨çš„æ ‘çŠ¶æ•°æ®ç»“æ„
    # â€œhtml.parserâ€æ˜¯è§£æå™¨ï¼Œè¿˜æœ‰lxmlã€html5lib
    # è¿™ä¸ª soup å¯¹è±¡ç°åœ¨æ˜¯æ•´ä¸ª HTML æ–‡æ¡£çš„ Pythonic è¡¨ç¤º,ã€‚
    # ä½ å¯ä»¥æŠŠå®ƒçœ‹ä½œä¸€ä¸ªå¤æ‚çš„ã€åµŒå¥—çš„ Python å¯¹è±¡ï¼Œå®ƒå®Œæ•´åœ°æ˜ å°„äº†åŸå§‹ HTML çš„æ ‡ç­¾ã€å±æ€§å’Œæ–‡æœ¬å†…å®¹ã€‚
    soup = BeautifulSoup(html_content, "html5lib")

    content_element = None
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„å…ƒæ•°æ®å­—å…¸
    metadata = {}

    # --- è°ƒåº¦ä¸­å¿ƒ ---
    # æ ¹æ® URL åˆ¤æ–­åº”è¯¥ä½¿ç”¨å“ªä¸ªå¤„ç†å™¨
    if "mp.weixin.qq.com" in url:
        content_element, metadata = _process_wechat_html(soup)
    else:
        content_element, metadata = _process_generic_html(soup, html_content)
    
    # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½æœªèƒ½æ‰¾åˆ°å†…å®¹ï¼Œåˆ™é€€å‡º
    if not content_element:
        print("âŒ æ‰€æœ‰è‡ªåŠ¨æ£€æµ‹æ–¹æ³•å‡å¤±è´¥ï¼Œæœªèƒ½æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸã€‚")
        return None
    
    print("âœ… æˆåŠŸæ‰¾åˆ°å†…å®¹å…ƒç´ ")
    
    # å¯¹æå–å‡ºçš„å†…å®¹è¿›è¡Œåå¤„ç†ï¼ˆä¾‹å¦‚ä¿®æ­£å›¾ç‰‡é“¾æ¥ï¼‰
    _post_process_content(content_element, url)

    # å°† HTML å…ƒç´ è½¬æ¢ä¸º Markdownæ–‡æ¡£å­—ç¬¦ä¸²ï¼Œmarkdownify()å‡½æ•°
    # heading_styleæ§åˆ¶ markdownify åœ¨å°† HTML æ ‡é¢˜æ ‡ç­¾ï¼ˆå¦‚ <h1>, <h2>, <h3> ç­‰ï¼‰è½¬æ¢ä¸º Markdown æ ‡é¢˜æ—¶æ‰€ä½¿ç”¨çš„æ ·å¼ã€‚
    # strip=['a'] å‚æ•°å¯ä»¥åœ¨è½¬æ¢å‰ç§»é™¤æ‰€æœ‰<a>æ ‡ç­¾ï¼Œä»¥è·å¾—æ›´å¹²å‡€çš„æ–‡æœ¬ã€‚
    markdown_text = markdownify(str(content_element), heading_style="ATX", strip=['a'])
    print(f"ğŸ”„ å·²å°† HTML (æ ‡é¢˜: {metadata.get('title', 'N/A')}) è½¬æ¢ä¸º Markdown")
    return markdown_text, metadata

# --- 8. åˆ›å»ºFront Matter ---
def _create_front_matter(metadata: dict, url: str) -> str:
    """æ ¹æ®æå–çš„å…ƒæ•°æ®ç”Ÿæˆ YAML Front Matter å­—ç¬¦ä¸²ã€‚"""
    # ä½¿ç”¨ isoformat() è·å–ç¬¦åˆ ISO 8601 æ ‡å‡†çš„æ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²
    created_time = datetime.now().isoformat()

    # æ„å»º Front Matter å­—å…¸
    front_matter_data = {
        "note_type": "æ–‡çŒ®ç¬”è®°",
        "content_type": "æ–°é—»æŠ¥é“",
        "created": created_time,
        "published": metadata.get("published", ""),
        "source": url,
        "author": metadata.get("author", ""),
        "description": metadata.get("description", ""),
        "site_name": metadata.get("site_name", ""),
    }

    # å°†å­—å…¸æ ¼å¼åŒ–ä¸º YAML å­—ç¬¦ä¸²
    yaml_lines = ["---"]
    for key, value in front_matter_data.items():
        yaml_lines.append(f"{key}: {value}")
    yaml_lines.append("---")

    return "\n".join(yaml_lines)


# --- 9. ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶ ---
def save_to_file(content: str, user_specified_path: str | None, page_title: str):
    """
    å°†å­—ç¬¦ä¸²å†…å®¹ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶ä¸­ã€‚
    :param content: è¦ä¿å­˜çš„å­—ç¬¦ä¸²å†…å®¹ã€‚
    :param user_specified_path: ç”¨æˆ·é€šè¿‡å‘½ä»¤è¡ŒæŒ‡å®šçš„è¾“å‡ºè·¯å¾„ï¼Œå¯èƒ½ä¸º Noneã€‚
    :param page_title: ä»ç½‘é¡µä¸­æå–çš„æ ‡é¢˜ï¼Œç”¨äºåœ¨ç”¨æˆ·æœªæŒ‡å®šè·¯å¾„æ—¶ç”Ÿæˆæ–‡ä»¶åã€‚
    """
    try:
        # å°†å‡€åŒ–æ–‡ä»¶åçš„é€»è¾‘å†…è”åˆ°è¿™é‡Œï¼šç§»é™¤æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦ï¼Œç„¶åæ·»åŠ  .md åç¼€
        sanitized_title_filename = re.sub(r'[\\/*?:"<>|]', "", str(page_title)).strip() + ".md"

        if user_specified_path: # ç”¨æˆ·æŒ‡å®šäº† -o å‚æ•°
            # åˆ¤æ–­ç”¨æˆ·æŒ‡å®šçš„æ˜¯ä¸€ä¸ªç›®å½•è¿˜æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
            if os.path.isdir(user_specified_path) or user_specified_path.endswith(('/', '\\')):
                # å¦‚æœç”¨æˆ·æŒ‡å®šçš„æ˜¯ä¸€ä¸ªç›®å½•ï¼Œåˆ™å°†æ ‡é¢˜ä½œä¸ºæ–‡ä»¶åä¸ç›®å½•ç»„åˆ
                output_path = os.path.join(user_specified_path, sanitized_title_filename)
            else:
                # å¦‚æœç”¨æˆ·æŒ‡å®šçš„æ˜¯ä¸€ä¸ªå®Œæ•´çš„æ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«æ–‡ä»¶åï¼‰ï¼Œåˆ™ç›´æ¥ä½¿ç”¨
                output_path = user_specified_path
        else:
            # ç”¨æˆ·æœªæŒ‡å®š -o å‚æ•°ï¼Œåˆ™åœ¨å½“å‰ç›®å½•ä½¿ç”¨æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
            output_path = sanitized_title_filename

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)

        # ä½¿ç”¨ with open() è¯­å¥ç¡®ä¿æ–‡ä»¶æ“ä½œçš„å®‰å…¨æ€§å’Œèµ„æºçš„è‡ªåŠ¨é‡Šæ”¾
        with open(output_path, "w", encoding="utf-8") as f: 
            f.write(content)
        print(f"ğŸ’¾ æ–‡ä»¶å·²æˆåŠŸä¿å­˜åˆ°: {os.path.abspath(output_path)}") # ä½¿ç”¨ os.path.abspath è·å–ç»å¯¹è·¯å¾„ï¼Œè®©è¾“å‡ºæ›´æ˜ç¡®
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")

# --- 10. ä»æ–‡ä»¶æå–URL ---
def _extract_urls_from_file(file_path: str) -> list[str]:
    """
    ä»ç»™å®šçš„æ–‡ä»¶ä¸­è¯»å–å†…å®¹ï¼Œå¹¶ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰ URLã€‚
    :param file_path: åŒ…å« URL çš„æ–‡ä»¶è·¯å¾„ã€‚
    :return: ä¸€ä¸ªåŒ…å«æ‰€æœ‰æ‰¾åˆ°çš„ URL çš„åˆ—è¡¨ã€‚
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰ http/https é“¾æ¥
        urls = re.findall(r'https?://[^\s<>"]+|www\.[^\s<>"]+', content)
        # ä½¿ç”¨ set å»é‡ï¼Œç„¶åè½¬å› list
        unique_urls = list(set(urls))
        print(f"ğŸ“„ ä»æ–‡ä»¶ '{file_path}' ä¸­æ‰¾åˆ° {len(unique_urls)} ä¸ªå”¯ä¸€é“¾æ¥ã€‚")
        return unique_urls
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return []
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return []

# --- 11. å¤„ç†äº¤äº’å¼ç™»å½• ---
async def handle_login(site: str):
    """
    å¤„ç†ç‰¹å®šç½‘ç«™çš„äº¤äº’å¼ç™»å½•æµç¨‹ï¼Œå¹¶ä¿å­˜ä¼šè¯çŠ¶æ€ã€‚
    :param site: ç½‘ç«™çš„æ ‡è¯†ç¬¦ï¼Œä¾‹å¦‚ 'wsj'ã€‚
    """
    site_configs = {
        'wsj': {
            'login_url': 'https://www.wsj.com/login',
            'env_var': 'WSJ_AUTH_STATE_PATH'
        }
        # æœªæ¥å¯ä»¥æ·»åŠ å…¶ä»–ç½‘ç«™çš„é…ç½®
    }

    if site not in site_configs:
        print(f"âŒ ä¸æ”¯æŒçš„ç™»å½•ç½‘ç«™: '{site}'ã€‚ç›®å‰åªæ”¯æŒ 'wsj'ã€‚")
        return

    config = site_configs[site]
    auth_file_path = os.environ.get(config['env_var'])

    if not auth_file_path:
        print(f"âŒ é”™è¯¯: è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ '{config['env_var']}' æ¥æŒ‡å®šä¼šè¯çŠ¶æ€æ–‡ä»¶çš„ä¿å­˜è·¯å¾„ã€‚")
        return

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False) # å¿…é¡»ä»¥éæ— å¤´æ¨¡å¼å¯åŠ¨
        context = await browser.new_context()
        page = await context.new_page()

        # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ³¨å…¥è„šæœ¬ä»¥éšè—è‡ªåŠ¨åŒ–ç‰¹å¾ ---
        # æŸäº›ç½‘ç«™ä¼šæ£€æµ‹ navigator.webdriver å±æ€§æ¥åˆ¤æ–­æ˜¯å¦ä¸ºè‡ªåŠ¨åŒ–æµè§ˆå™¨ã€‚
        # æˆ‘ä»¬åœ¨é¡µé¢åŠ è½½ä»»ä½•è„šæœ¬ä¹‹å‰ï¼Œé€šè¿‡ add_init_script å°†è¿™ä¸ªå±æ€§çš„å€¼ä¼ªè£…æˆ falseã€‚
        await page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => false})")

        await page.goto(config['login_url'])
        
        print("\n" + "="*50)
        print("ğŸš€ äº¤äº’å¼ç™»å½•æµç¨‹å·²å¯åŠ¨ ğŸš€")
        print(f"è¯·åœ¨å¼¹å‡ºçš„æµè§ˆå™¨çª—å£ä¸­æ‰‹åŠ¨å®Œæˆ '{site.upper()}' çš„ç™»å½•ã€‚")
        print("ç™»å½•æˆåŠŸåï¼Œè¯·ä¸è¦å…³é—­æµè§ˆå™¨ï¼Œå›åˆ°è¿™é‡ŒæŒ‰ä¸‹ 'Enter' é”®ç»§ç»­...")
        print("="*50 + "\n")
        
        input() # ç­‰å¾…ç”¨æˆ·æŒ‰ Enter

        # ä¿å­˜å®Œæ•´çš„ä¼šè¯çŠ¶æ€åˆ°æŒ‡å®šæ–‡ä»¶
        await context.storage_state(path=auth_file_path)
        print(f"âœ… ä¼šè¯çŠ¶æ€å·²æˆåŠŸä¿å­˜åˆ°: {auth_file_path}")

        await browser.close()


# --- 12. ä¸»ç¨‹åºå…¥å£ ---
async def main():
    """
    ç¨‹åºçš„ä¸»å¼‚æ­¥å…¥å£ï¼Œè´Ÿè´£ç¼–æ’æ•´ä¸ªæŠ“å–ã€è½¬æ¢å’Œä¿å­˜çš„å·¥ä½œæµã€‚
    """
    # ä½¿ç”¨ argparse è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="ä¸€ä¸ªé€šç”¨çš„ç½‘é¡µå†…å®¹æŠ“å–å¹¶è½¬æ¢ä¸º Markdown çš„å·¥å…·ã€‚")
    parser.add_argument("input_source", nargs='?', default=None, help="è¦æŠ“å–çš„ç›®æ ‡ç½‘é¡µ URLï¼Œæˆ–åŒ…å«å¤šä¸ª URL çš„æ–‡ä»¶è·¯å¾„ã€‚")
    # ä¿®æ”¹-oå‚æ•°ï¼Œä½¿å…¶é»˜è®¤å€¼ä¸ºNoneï¼Œä»¥ä¾¿æˆ‘ä»¬åˆ¤æ–­ç”¨æˆ·æ˜¯å¦çœŸçš„è¾“å…¥äº†å®ƒ
    parser.add_argument("-o", "--output", help="è¾“å‡ºçš„ Markdown æ–‡ä»¶è·¯å¾„ã€‚å¦‚æœæœªæä¾›ï¼Œå°†æ ¹æ®ç½‘é¡µæ ‡é¢˜è‡ªåŠ¨ç”Ÿæˆã€‚")
    parser.add_argument("--login", help="å¯åŠ¨äº¤äº’å¼ç™»å½•æµç¨‹ä»¥ä¿å­˜ç‰¹å®šç½‘ç«™çš„ä¼šè¯çŠ¶æ€ã€‚ä¾‹å¦‚: --login wsj")
    args = parser.parse_args()

    # --- æ¨¡å¼è°ƒåº¦ ---
    if args.login:
        # å¦‚æœç”¨æˆ·æŒ‡å®šäº† --login å‚æ•°ï¼Œåˆ™æ‰§è¡Œç™»å½•æµç¨‹å¹¶é€€å‡º
        await handle_login(args.login)
        return

    if not args.input_source:
        parser.error("é”™è¯¯: å¿…é¡»æä¾›ä¸€ä¸ª URL æˆ–æ–‡ä»¶è·¯å¾„ä½œä¸ºè¾“å…¥æºï¼Œæˆ–è€…ä½¿ç”¨ --login é€‰é¡¹ã€‚")
        return

    urls_to_process = []
    # åˆ¤æ–­è¾“å…¥æ˜¯æ–‡ä»¶è¿˜æ˜¯ URL
    if os.path.isfile(args.input_source):
        urls_to_process = _extract_urls_from_file(args.input_source)
    else:
        # å¦‚æœä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„ï¼Œåˆ™å‡å®šå®ƒæ˜¯ä¸€ä¸ª URL
        urls_to_process.append(args.input_source)

    for i, url in enumerate(urls_to_process):
        print(f"\n--- æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{len(urls_to_process)} ä¸ªé“¾æ¥: {url} ---")
        # 1. æå–
        html_content = await fetch_html_from_url(url)
        if not html_content:
            continue # å¦‚æœå½“å‰ URL å¤±è´¥ï¼Œåˆ™è·³è¿‡å¹¶ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª

        # 2. è½¬æ¢
        conversion_result = convert_html_to_markdown(html_content, url)
        if not conversion_result:
            continue
        
        markdown_text, metadata = conversion_result

        # 2.5. ç”Ÿæˆ Front Matter
        front_matter = _create_front_matter(metadata, url)

        # å°† Front Matter å’Œæ­£æ–‡å†…å®¹æ‹¼æ¥èµ·æ¥
        final_content = f"{front_matter}{SUMMARY_TEMPLATE}{markdown_text}"

        # 3. ä¿å­˜
        save_to_file(final_content, args.output, metadata.get("title", "Untitled"))
        
        # --- æ–°å¢ï¼šåœ¨å¤„ç†å¤šä¸ªé“¾æ¥æ—¶ï¼Œå¢åŠ ä¸€ä¸ªè¾ƒé•¿çš„éšæœºç­‰å¾…ï¼Œä»¥é¿å…è®¿é—®é¢‘ç‡è¿‡å¿« ---
        if len(urls_to_process) > 1 and i < len(urls_to_process) - 1:
            long_wait = random.uniform(10, 30)
            print(f"\nâ³ æ‰¹é‡å¤„ç†é—´éš”ï¼Œéšæœºç­‰å¾… {long_wait:.2f} ç§’...")
            await asyncio.sleep(long_wait)

if __name__ == "__main__":
    # å› ä¸ºæˆ‘ä»¬çš„æ ¸å¿ƒå‡½æ•°æ˜¯å¼‚æ­¥çš„ï¼Œæ‰€ä»¥éœ€è¦ä½¿ç”¨ asyncio.run() æ¥å¯åŠ¨å®ƒ
    asyncio.run(main())
