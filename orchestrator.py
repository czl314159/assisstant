# d:\Documents\Assistant\orchestrator.py
from ai_service import AIAssistantService
from memory_store import MemoryStore
import os

class Orchestrator:
    """
    è°ƒåº¦å±‚ (Orchestrator)
    - è´Ÿè´£æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼Œä½œä¸ºAIçš„â€œå¤§è„‘â€ã€‚
    - æ¥æ”¶æ¥è‡ªäº¤äº’å±‚çš„è¯·æ±‚ï¼Œå¹¶å†³å®šå¦‚ä½•å“åº”ã€‚
    - ç®¡ç†ä¸AIæœåŠ¡å’Œè®°å¿†å­˜å‚¨çš„äº¤äº’ã€‚
    """
    def __init__(self, api_key, model_name, api_url, temperature, memory_root):
        self.ai_service = AIAssistantService(
            api_key=api_key,
            model_name=model_name,
            api_url=api_url,
            temperature=temperature,
        )
        self.memory_store = MemoryStore(root_dir=memory_root)

    def handle_cli_request(self, user_input, conversation_history, memory_mode, session_id, file_context=None):
        """å¤„ç†æ¥è‡ªCLIçš„å•æ¬¡è¯·æ±‚ï¼Œå¹¶æµå¼æ‰“å°å“åº”ã€‚"""
        
        if file_context:
            final_input = f"""è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹æ¥å›ç­”æˆ‘çš„é—®é¢˜ã€‚
---
æ–‡æ¡£å†…å®¹:
{file_context}
---
æˆ‘çš„é—®é¢˜æ˜¯ï¼š{user_input}
"""
        else:
            final_input = user_input
        
        conversation_history.append({"role": "user", "content": final_input})

        if memory_mode == 'no':
            history_to_send = [conversation_history[-1]]
        else:
            history_to_send = conversation_history

        print(f"AIåŠ©æ‰‹ï¼š", end="")
        full_response = ""
        has_error = False
        for chunk in self.ai_service.stream_chat_completion(history_to_send):
            if "ç½‘ç»œé”™è¯¯" in chunk or "æœªçŸ¥é”™è¯¯" in chunk:
                has_error = True
            full_response += chunk
            print(chunk, end="", flush=True)
        print()

        if not has_error:
            conversation_history.append({"role": "assistant", "content": full_response})
            if memory_mode == 'long':
                self.memory_store.save(session_id, conversation_history)
        
        return conversation_history

    def handle_gui_request(self, user_input, conversation_state, session_id):
        """å¤„ç†æ¥è‡ªGUIçš„æµå¼è¯·æ±‚ï¼Œä½œä¸ºç”Ÿæˆå™¨è¿”å›å“åº”ã€‚"""
        session_id = self.memory_store.normalize_session_id(session_id)
        conversation_state.append({"role": "user", "content": user_input})

        full_response = ""
        has_error = False
        for chunk in self.ai_service.stream_chat_completion(conversation_state):
            if "ç½‘ç»œé”™è¯¯" in chunk or "æœªçŸ¥é”™è¯¯" in chunk:
                has_error = True
            full_response += chunk
            yield full_response

        if not has_error:
            conversation_state.append({"role": "assistant", "content": full_response})
            self.memory_store.save(session_id, conversation_state)
    
    def switch_gui_session(self, requested_session, conversation_history, current_session_id):
        """å¤„ç†GUIçš„ä¼šè¯åˆ‡æ¢è¯·æ±‚ã€‚"""
        current_session_id = self.memory_store.normalize_session_id(current_session_id)
        if conversation_history:
            self.memory_store.save(current_session_id, conversation_history)

        new_session = self.memory_store.normalize_session_id(requested_session)
        new_history = self.memory_store.load(new_session)
        print(f"ğŸ—„ å·²åˆ‡æ¢åˆ°ä¼šè¯ '{new_session}'ï¼Œå…± {len(new_history)} æ¡æ¶ˆæ¯ã€‚")
        return new_session, new_history

    def load_memory(self, session_id):
        """åŠ è½½æŒ‡å®šä¼šè¯çš„å†å²è®°å½•ã€‚"""
        return self.memory_store.load(session_id)
    
    def save_memory(self, session_id, history):
        """ä¿å­˜æŒ‡å®šä¼šè¯çš„å†å²è®°å½•ã€‚"""
        self.memory_store.save(session_id, history)

    def normalize_session_id(self, session_id):
        """è§„èŒƒåŒ–ä¼šè¯IDï¼Œç§»é™¤ä¸å®‰å…¨å­—ç¬¦ã€‚"""
        return self.memory_store.normalize_session_id(session_id)